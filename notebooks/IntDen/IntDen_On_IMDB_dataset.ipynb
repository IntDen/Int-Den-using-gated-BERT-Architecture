{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042819,
     "end_time": "2021-01-23T04:38:26.258205",
     "exception": false,
     "start_time": "2021-01-23T04:38:26.215386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### References\n",
    "\n",
    "* https://github.com/huggingface/transformers/blob/master/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py\n",
    "\n",
    "* https://github.com/huggingface/transformers/blob/c4d4e8bdbd25d9463d41de6398940329c89b7fb6/src/transformers/generation_utils.py#L101\n",
    "\n",
    "* https://github.com/hiyoung123/SoftMaskedBert/blob/master/train.py\n",
    "\n",
    "* https://github.com/atulkum/pointer_summarizer/blob/master/training_ptr_gen/model.py\n",
    "\n",
    "* https://github.com/nyu-dl/bert-gen/blob/master/bert-babble.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:38:26.348399Z",
     "iopub.status.busy": "2021-01-23T04:38:26.347585Z",
     "iopub.status.idle": "2021-01-23T04:38:38.686643Z",
     "shell.execute_reply": "2021-01-23T04:38:38.685991Z"
    },
    "papermill": {
     "duration": 12.386958,
     "end_time": "2021-01-23T04:38:38.686768",
     "exception": false,
     "start_time": "2021-01-23T04:38:26.299810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.0.1\r\n",
      "  Downloading transformers-4.0.1-py3-none-any.whl (1.4 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 4.3 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.0.1) (2020.4.4)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==4.0.1) (1.18.5)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.0.1) (3.0.10)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.0.1) (20.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.0.1) (4.45.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.0.1) (2.23.0)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.0.1) (0.0.43)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.0.1) (1.14.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.0.1) (2.4.7)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.0.1) (1.25.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.0.1) (2020.12.5)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.0.1) (2.9)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.0.1) (3.0.4)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.0.1) (0.14.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.0.1) (2020.4.4)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.0.1) (4.45.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.0.1) (1.14.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.0.1) (7.1.1)\r\n",
      "Collecting tokenizers==0.9.4\r\n",
      "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 37.8 MB/s \r\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.9.3\r\n",
      "    Uninstalling tokenizers-0.9.3:\r\n",
      "      Successfully uninstalled tokenizers-0.9.3\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 3.5.1\r\n",
      "    Uninstalling transformers-3.5.1:\r\n",
      "      Successfully uninstalled transformers-3.5.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "allennlp 1.2.2 requires transformers<3.6,>=3.4, but you have transformers 4.0.1 which is incompatible.\u001b[0m\r\n",
      "Successfully installed tokenizers-0.9.4 transformers-4.0.1\r\n",
      "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:38:38.792333Z",
     "iopub.status.busy": "2021-01-23T04:38:38.791143Z",
     "iopub.status.idle": "2021-01-23T04:38:48.766160Z",
     "shell.execute_reply": "2021-01-23T04:38:48.767131Z"
    },
    "papermill": {
     "duration": 10.032668,
     "end_time": "2021-01-23T04:38:48.767357",
     "exception": false,
     "start_time": "2021-01-23T04:38:38.734689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, \\\n",
    "                         AutoConfig, EncoderDecoderModel, BertForMaskedLM\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from transformers.file_utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.utils import logging\n",
    "from transformers.models.encoder_decoder.configuration_encoder_decoder import EncoderDecoderConfig\n",
    "from transformers.models.auto.modeling_auto import AutoModel, AutoModelForCausalLM, AutoConfig, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:38:48.953161Z",
     "iopub.status.busy": "2021-01-23T04:38:48.951231Z",
     "iopub.status.idle": "2021-01-23T04:38:48.953868Z",
     "shell.execute_reply": "2021-01-23T04:38:48.954363Z"
    },
    "papermill": {
     "duration": 0.117476,
     "end_time": "2021-01-23T04:38:48.954501",
     "exception": false,
     "start_time": "2021-01-23T04:38:48.837025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed_val = 66\n",
    "MIN_LEN = 20\n",
    "MAX_LEN = 400\n",
    "batch_size = 4\n",
    "EPOCHS = 20\n",
    "early_stopping_rounds = 5\n",
    "lr = 5e-5\n",
    "\n",
    "encoder_path = 'bert-base-uncased'\n",
    "decoder_path = 'bert-base-uncased'\n",
    "\n",
    "teacher_forcing=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:38:49.083518Z",
     "iopub.status.busy": "2021-01-23T04:38:49.082700Z",
     "iopub.status.idle": "2021-01-23T04:38:49.086819Z",
     "shell.execute_reply": "2021-01-23T04:38:49.086291Z"
    },
    "papermill": {
     "duration": 0.075631,
     "end_time": "2021-01-23T04:38:49.086925",
     "exception": false,
     "start_time": "2021-01-23T04:38:49.011294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:38:49.194272Z",
     "iopub.status.busy": "2021-01-23T04:38:49.193601Z",
     "iopub.status.idle": "2021-01-23T04:38:49.198189Z",
     "shell.execute_reply": "2021-01-23T04:38:49.197718Z"
    },
    "papermill": {
     "duration": 0.058603,
     "end_time": "2021-01-23T04:38:49.198289",
     "exception": false,
     "start_time": "2021-01-23T04:38:49.139686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:38:49.304737Z",
     "iopub.status.busy": "2021-01-23T04:38:49.302771Z",
     "iopub.status.idle": "2021-01-23T04:38:49.305491Z",
     "shell.execute_reply": "2021-01-23T04:38:49.306026Z"
    },
    "papermill": {
     "duration": 0.057209,
     "end_time": "2021-01-23T04:38:49.306157",
     "exception": false,
     "start_time": "2021-01-23T04:38:49.248948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_input = pd.read_csv('../input/denoising/ALTA_2017/train_input.csv')\n",
    "# train_output = pd.read_csv('../input/denoising/ALTA_2017/train_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:38:49.416806Z",
     "iopub.status.busy": "2021-01-23T04:38:49.415990Z",
     "iopub.status.idle": "2021-01-23T04:39:01.745058Z",
     "shell.execute_reply": "2021-01-23T04:39:01.744355Z"
    },
    "papermill": {
     "duration": 12.386563,
     "end_time": "2021-01-23T04:39:01.745182",
     "exception": false,
     "start_time": "2021-01-23T04:38:49.358619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/imdb-noiseddata/IMDB_denoisedData.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/imdb-noiseddata'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        imdb_data =  pd.read_csv(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:01.863667Z",
     "iopub.status.busy": "2021-01-23T04:39:01.862980Z",
     "iopub.status.idle": "2021-01-23T04:39:01.869511Z",
     "shell.execute_reply": "2021-01-23T04:39:01.868944Z"
    },
    "papermill": {
     "duration": 0.075242,
     "end_time": "2021-01-23T04:39:01.869615",
     "exception": false,
     "start_time": "2021-01-23T04:39:01.794373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'review', 'char_augumented', 'keyborad_augumented',\n",
       "       'random_augumented', 'substitute_augumented', 'swap_augumented',\n",
       "       'delete_augumented', 'spell_augumented', 'Synonym_augumented',\n",
       "       'swapWord_augumented'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:01.982074Z",
     "iopub.status.busy": "2021-01-23T04:39:01.981248Z",
     "iopub.status.idle": "2021-01-23T04:39:01.989494Z",
     "shell.execute_reply": "2021-01-23T04:39:01.988985Z"
    },
    "papermill": {
     "duration": 0.069327,
     "end_time": "2021-01-23T04:39:01.989595",
     "exception": false,
     "start_time": "2021-01-23T04:39:01.920268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "imdb_dataset_v1 = imdb_data[['delete_augumented','review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:02.095733Z",
     "iopub.status.busy": "2021-01-23T04:39:02.094947Z",
     "iopub.status.idle": "2021-01-23T04:39:02.098292Z",
     "shell.execute_reply": "2021-01-23T04:39:02.098840Z"
    },
    "papermill": {
     "duration": 0.059191,
     "end_time": "2021-01-23T04:39:02.098971",
     "exception": false,
     "start_time": "2021-01-23T04:39:02.039780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['delete_augumented', 'review'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset_v1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:02.213280Z",
     "iopub.status.busy": "2021-01-23T04:39:02.212162Z",
     "iopub.status.idle": "2021-01-23T04:39:02.215517Z",
     "shell.execute_reply": "2021-01-23T04:39:02.214969Z"
    },
    "papermill": {
     "duration": 0.065367,
     "end_time": "2021-01-23T04:39:02.215630",
     "exception": false,
     "start_time": "2021-01-23T04:39:02.150263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "imdb_dataset_v1 = imdb_dataset_v1.rename(columns={\"review\": \"solution\", \"delete_augumented\": \"original\"}, errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:02.324040Z",
     "iopub.status.busy": "2021-01-23T04:39:02.323148Z",
     "iopub.status.idle": "2021-01-23T04:39:02.327328Z",
     "shell.execute_reply": "2021-01-23T04:39:02.326832Z"
    },
    "papermill": {
     "duration": 0.0598,
     "end_time": "2021-01-23T04:39:02.327432",
     "exception": false,
     "start_time": "2021-01-23T04:39:02.267632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['original', 'solution'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset_v1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:02.465685Z",
     "iopub.status.busy": "2021-01-23T04:39:02.455482Z",
     "iopub.status.idle": "2021-01-23T04:39:02.671253Z",
     "shell.execute_reply": "2021-01-23T04:39:02.670734Z"
    },
    "papermill": {
     "duration": 0.293115,
     "end_time": "2021-01-23T04:39:02.671379",
     "exception": false,
     "start_time": "2021-01-23T04:39:02.378264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "imdb_dataset_v1['original'] = imdb_dataset_v1['original'].map(lambda x: x.replace('<br /> <br />',\"\"))\n",
    "imdb_dataset_v1['solution'] = imdb_dataset_v1['solution'].map(lambda x: x.replace('<br /><br />',\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:02.783936Z",
     "iopub.status.busy": "2021-01-23T04:39:02.783192Z",
     "iopub.status.idle": "2021-01-23T04:39:02.795242Z",
     "shell.execute_reply": "2021-01-23T04:39:02.794646Z"
    },
    "papermill": {
     "duration": 0.074359,
     "end_time": "2021-01-23T04:39:02.795350",
     "exception": false,
     "start_time": "2021-01-23T04:39:02.720991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>solution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production.  The filming te...</td>\n",
       "      <td>A wonderful little production. The filming tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend tm...</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bsially there ' s a family whee a little boy (...</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei ' s \" ove in the Time of Money \"...</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49994</th>\n",
       "      <td>This is your typical junk comedy.  There are a...</td>\n",
       "      <td>This is your typical junk comedy.There are alm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I ' m going to have to disagree wth the previo...</td>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                original  \\\n",
       "0      One of the other reviewers has mentioned that ...   \n",
       "1      A wonderful little production.  The filming te...   \n",
       "2      I thought this was a wonderful way to spend tm...   \n",
       "3      Bsially there ' s a family whee a little boy (...   \n",
       "4      Petter Mattei ' s \" ove in the Time of Money \"...   \n",
       "...                                                  ...   \n",
       "49994  This is your typical junk comedy.  There are a...   \n",
       "49995  I thought this movie did a down right good job...   \n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...   \n",
       "49997  I am a Catholic taught in parochial elementary...   \n",
       "49998  I ' m going to have to disagree wth the previo...   \n",
       "\n",
       "                                                solution  \n",
       "0      One of the other reviewers has mentioned that ...  \n",
       "1      A wonderful little production. The filming tec...  \n",
       "2      I thought this was a wonderful way to spend ti...  \n",
       "3      Basically there's a family where a little boy ...  \n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  \n",
       "...                                                  ...  \n",
       "49994  This is your typical junk comedy.There are alm...  \n",
       "49995  I thought this movie did a down right good job...  \n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  \n",
       "49997  I am a Catholic taught in parochial elementary...  \n",
       "49998  I'm going to have to disagree with the previou...  \n",
       "\n",
       "[49999 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset_v1.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:02.906364Z",
     "iopub.status.busy": "2021-01-23T04:39:02.905712Z",
     "iopub.status.idle": "2021-01-23T04:39:02.910153Z",
     "shell.execute_reply": "2021-01-23T04:39:02.909626Z"
    },
    "papermill": {
     "duration": 0.06196,
     "end_time": "2021-01-23T04:39:02.910282",
     "exception": false,
     "start_time": "2021-01-23T04:39:02.848322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = imdb_dataset_v1[:9000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:03.020957Z",
     "iopub.status.busy": "2021-01-23T04:39:03.019158Z",
     "iopub.status.idle": "2021-01-23T04:39:03.021676Z",
     "shell.execute_reply": "2021-01-23T04:39:03.022220Z"
    },
    "papermill": {
     "duration": 0.059543,
     "end_time": "2021-01-23T04:39:03.022342",
     "exception": false,
     "start_time": "2021-01-23T04:39:02.962799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:03.129091Z",
     "iopub.status.busy": "2021-01-23T04:39:03.128239Z",
     "iopub.status.idle": "2021-01-23T04:39:03.131407Z",
     "shell.execute_reply": "2021-01-23T04:39:03.130904Z"
    },
    "papermill": {
     "duration": 0.056418,
     "end_time": "2021-01-23T04:39:03.131520",
     "exception": false,
     "start_time": "2021-01-23T04:39:03.075102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_input.original.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:03.235531Z",
     "iopub.status.busy": "2021-01-23T04:39:03.234586Z",
     "iopub.status.idle": "2021-01-23T04:39:03.237778Z",
     "shell.execute_reply": "2021-01-23T04:39:03.237260Z"
    },
    "papermill": {
     "duration": 0.057012,
     "end_time": "2021-01-23T04:39:03.237874",
     "exception": false,
     "start_time": "2021-01-23T04:39:03.180862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:03.342067Z",
     "iopub.status.busy": "2021-01-23T04:39:03.341277Z",
     "iopub.status.idle": "2021-01-23T04:39:03.344014Z",
     "shell.execute_reply": "2021-01-23T04:39:03.344526Z"
    },
    "papermill": {
     "duration": 0.056812,
     "end_time": "2021-01-23T04:39:03.344642",
     "exception": false,
     "start_time": "2021-01-23T04:39:03.287830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train = pd.merge(train_input, train_output, how='inner')\n",
    "# #train['solution'] = train.solution.apply(lambda x: re.sub('[^a-z0-9 ]', '', x.strip().lower()))\n",
    "# #train['original'] = train.original.apply(lambda x: re.sub(r'[^a-z0-9 ]', '', x.strip().lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:03.481338Z",
     "iopub.status.busy": "2021-01-23T04:39:03.471047Z",
     "iopub.status.idle": "2021-01-23T04:39:03.615421Z",
     "shell.execute_reply": "2021-01-23T04:39:03.615953Z"
    },
    "papermill": {
     "duration": 0.22098,
     "end_time": "2021-01-23T04:39:03.616078",
     "exception": false,
     "start_time": "2021-01-23T04:39:03.395098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9000.000000\n",
       "mean      249.532222\n",
       "std       185.269240\n",
       "min        14.000000\n",
       "25%       136.000000\n",
       "50%       186.000000\n",
       "75%       305.000000\n",
       "max      1959.000000\n",
       "Name: original, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.original.apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:03.720727Z",
     "iopub.status.busy": "2021-01-23T04:39:03.719894Z",
     "iopub.status.idle": "2021-01-23T04:39:04.778065Z",
     "shell.execute_reply": "2021-01-23T04:39:04.776866Z"
    },
    "papermill": {
     "duration": 1.111249,
     "end_time": "2021-01-23T04:39:04.778187",
     "exception": false,
     "start_time": "2021-01-23T04:39:03.666938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e328f06d7464078bf296121a2fa766b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc158a2e148456e9449f7ce64306a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8226114751a466fa7af7f5d9d393c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_tokenizer = AutoTokenizer.from_pretrained(encoder_path)\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(decoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:04.889881Z",
     "iopub.status.busy": "2021-01-23T04:39:04.888027Z",
     "iopub.status.idle": "2021-01-23T04:39:04.890638Z",
     "shell.execute_reply": "2021-01-23T04:39:04.891166Z"
    },
    "papermill": {
     "duration": 0.060701,
     "end_time": "2021-01-23T04:39:04.891289",
     "exception": false,
     "start_time": "2021-01-23T04:39:04.830588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "\n",
    "for train_index, val_index in kf.split(train):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:05.001483Z",
     "iopub.status.busy": "2021-01-23T04:39:05.000667Z",
     "iopub.status.idle": "2021-01-23T04:39:05.005751Z",
     "shell.execute_reply": "2021-01-23T04:39:05.005160Z"
    },
    "papermill": {
     "duration": 0.0615,
     "end_time": "2021-01-23T04:39:05.005874",
     "exception": false,
     "start_time": "2021-01-23T04:39:04.944374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val = train.iloc[val_index]\n",
    "train = train.iloc[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:05.124377Z",
     "iopub.status.busy": "2021-01-23T04:39:05.123551Z",
     "iopub.status.idle": "2021-01-23T04:39:28.379310Z",
     "shell.execute_reply": "2021-01-23T04:39:28.379912Z"
    },
    "papermill": {
     "duration": 23.321521,
     "end_time": "2021-01-23T04:39:28.380055",
     "exception": false,
     "start_time": "2021-01-23T04:39:05.058534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7200/7200 [00:08<00:00, 860.82it/s]\n",
      "100%|██████████| 7200/7200 [00:09<00:00, 792.54it/s]\n",
      "100%|██████████| 1800/1800 [00:02<00:00, 878.90it/s]\n",
      "100%|██████████| 1800/1800 [00:02<00:00, 798.27it/s]\n"
     ]
    }
   ],
   "source": [
    "trainX = torch.Tensor(np.asarray([encoder_tokenizer.encode(i, max_length=MAX_LEN, truncation=True, padding='max_length', add_special_tokens=True) \\\n",
    "                                  for i in tqdm(train.original.values)]))\n",
    "trainy = torch.Tensor(np.asarray([decoder_tokenizer.encode(i, max_length=MAX_LEN, truncation=True, padding='max_length', add_special_tokens=True) \\\n",
    "                                  for i in tqdm(train.solution.values)]))\n",
    "\n",
    "valX = torch.Tensor(np.asarray([encoder_tokenizer.encode(i, max_length=MAX_LEN, truncation=True, padding='max_length', add_special_tokens=True) \\\n",
    "                                for i in tqdm(val.original.values)]))\n",
    "valy = torch.Tensor(np.asarray([decoder_tokenizer.encode(i, max_length=MAX_LEN, truncation=True, padding='max_length', add_special_tokens=True) \\\n",
    "                                for i in tqdm(val.solution.values)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:28.641015Z",
     "iopub.status.busy": "2021-01-23T04:39:28.640169Z",
     "iopub.status.idle": "2021-01-23T04:39:28.645166Z",
     "shell.execute_reply": "2021-01-23T04:39:28.644660Z"
    },
    "papermill": {
     "duration": 0.135482,
     "end_time": "2021-01-23T04:39:28.645261",
     "exception": false,
     "start_time": "2021-01-23T04:39:28.509779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7200, 400]),\n",
       " torch.Size([7200, 400]),\n",
       " torch.Size([1800, 400]),\n",
       " torch.Size([1800, 400]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape, trainy.shape, valX.shape, valy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:28.907940Z",
     "iopub.status.busy": "2021-01-23T04:39:28.906889Z",
     "iopub.status.idle": "2021-01-23T04:39:28.942689Z",
     "shell.execute_reply": "2021-01-23T04:39:28.943516Z"
    },
    "papermill": {
     "duration": 0.171554,
     "end_time": "2021-01-23T04:39:28.943722",
     "exception": false,
     "start_time": "2021-01-23T04:39:28.772168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "trainX = torch.tensor(trainX, dtype=torch.long)\n",
    "trainy = torch.tensor(trainy, dtype=torch.long)\n",
    "valX = torch.tensor(valX, dtype=torch.long)\n",
    "valy = torch.tensor(valy, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:29.257335Z",
     "iopub.status.busy": "2021-01-23T04:39:29.246424Z",
     "iopub.status.idle": "2021-01-23T04:39:29.287640Z",
     "shell.execute_reply": "2021-01-23T04:39:29.287128Z"
    },
    "papermill": {
     "duration": 0.205986,
     "end_time": "2021-01-23T04:39:29.287747",
     "exception": false,
     "start_time": "2021-01-23T04:39:29.081761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "_CONFIG_FOR_DOC = \"EncoderDecoderConfig\"\n",
    "\n",
    "ENCODER_DECODER_START_DOCSTRING = r\"\"\"\n",
    "    This class can be used to initialize a sequence-tsequencece model with any pretrained autoencoding model as the\n",
    "    encoder and any pretrained autoregressive model as the decoder. The encoder is loaded via\n",
    "    :meth:`~transformers.AutoModel.from_pretrained` function and the decoder is loaded via\n",
    "    :meth:`~transformers.AutoModelForCausalLM.from_pretrained` function. Cross-attention layers are automatically added\n",
    "    to the decoder and should be fine-tuned on a downstream generative task, like summarization.\n",
    "    The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation\n",
    "    tasks was shown in `Leveraging Pre-trained Checkpoints for Sequence Generation Tasks\n",
    "    <https://arxiv.org/abs/1907.12461>`__ by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi\n",
    "    Zhou, Wei Li, Peter J. Liu.\n",
    "    After such an Encoder Decoder model has been trained/fine-tuned, it can be saved/loaded just like any other models\n",
    "    (see the examples for more information).\n",
    "    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n",
    "    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n",
    "    pruning heads etc.)\n",
    "    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__\n",
    "    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to\n",
    "    general usage and behavior.\n",
    "    Parameters:\n",
    "        config (:class:`~transformers.T5Config`): Model configuration class with all the parameters of the model.\n",
    "            Initializing with a config file does not load the weights associated with the model, only the\n",
    "            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n",
    "            weights.\n",
    "\"\"\"\n",
    "\n",
    "ENCODER_DECODER_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary.\n",
    "            Indices can be obtained using :class:`~transformers.PreTrainedTokenizer`. See\n",
    "            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
    "            details.\n",
    "            `What are input IDs? <../glossary.html#input-ids>`__\n",
    "        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "            `What are attention masks? <../glossary.html#attention-mask>`__\n",
    "        decoder_input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, target_sequence_length)`, `optional`):\n",
    "            Provide for sequence to sequence training to the decoder. Indices can be obtained using\n",
    "            :class:`~transformers.PretrainedTokenizer`. See :meth:`transformers.PreTrainedTokenizer.encode` and\n",
    "            :meth:`transformers.PreTrainedTokenizer.__call__` for details.\n",
    "        decoder_attention_mask (:obj:`torch.BoolTensor` of shape :obj:`(batch_size, tgt_seq_len)`, `optional`):\n",
    "            Default behavior: generate a tensor that ignores pad tokens in :obj:`decoder_input_ids`. Causal mask will\n",
    "            also be used by default.\n",
    "        encoder_outputs (:obj:`tuple(torch.FloatTensor)`, `optional`):\n",
    "            This tuple must consist of (:obj:`last_hidden_state`, `optional`: :obj:`hidden_states`, `optional`:\n",
    "            :obj:`attentions`) :obj:`last_hidden_state` (:obj:`torch.FloatTensor` of shape :obj:`(batch_size,\n",
    "            sequence_length, hidden_size)`) is a tensor of hidden-states at the output of the last layer of the\n",
    "            encoder. Used in the cross-attention of the decoder.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
    "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
    "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
    "        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n",
    "            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n",
    "            vectors than the model's internal embedding lookup matrix.\n",
    "        decoder_inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, target_sequence_length, hidden_size)`, `optional`):\n",
    "            Optionally, instead of passing :obj:`decoder_input_ids` you can choose to directly pass an embedded\n",
    "            representation. This is useful if you want more control over how to convert :obj:`decoder_input_ids`\n",
    "            indices into associated vectors than the model's internal embedding lookup matrix.\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the masked language modeling loss for the decoder. Indices should be in ``[-100, 0,\n",
    "            ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
    "        use_cache (:obj:`bool`, `optional`):\n",
    "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
    "            decoding (see :obj:`past_key_values`).\n",
    "        output_attentions (:obj:`bool`, `optional`):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (:obj:`bool`, `optional`):\n",
    "            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (:obj:`bool`, `optional`):\n",
    "            If set to ``True``, the model will return a :class:`~transformers.file_utils.Seq2SeqLMOutput` instead of a\n",
    "            plain tuple.\n",
    "        kwargs: (`optional`) Remaining dictionary of keyword arguments. Keyword arguments come in two flavors:\n",
    "            - Without a prefix which will be input as ``**encoder_kwargs`` for the encoder forward function.\n",
    "            - With a `decoder_` prefix which will be input as ``**decoder_kwargs`` for the decoder forward function.\n",
    "\"\"\"\n",
    "\n",
    "@add_start_docstrings(ENCODER_DECODER_START_DOCSTRING)\n",
    "class EncoderDecoderModelWithGates(PreTrainedModel):\n",
    "    r\"\"\"\n",
    "    :class:`~transformers.EncoderDecoder` is a generic model class that will be instantiated as a transformer\n",
    "    architecture with one of the base model classes of the library as encoder and another one as decoder when created\n",
    "    with the :meth`~transformers.AutoModel.from_pretrained` class method for the encoder and\n",
    "    :meth`~transformers.AutoModelForCausalLM.from_pretrained` class method for the decoder.\n",
    "    \"\"\"\n",
    "    config_class = EncoderDecoderConfig\n",
    "    base_model_prefix = \"encoder_decoder\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Optional[PretrainedConfig] = None,\n",
    "        encoder: Optional[PreTrainedModel] = None,\n",
    "        decoder: Optional[PreTrainedModel] = None,\n",
    "    ):\n",
    "        \n",
    "        assert config is not None or (\n",
    "            encoder is not None and decoder is not None\n",
    "        ), \"Either a configuration or an Encoder and a decoder has to be provided\"\n",
    "        if config is None:\n",
    "            config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder.config, decoder.config)\n",
    "        else:\n",
    "            assert isinstance(config, self.config_class), \"config: {} has to be of type {}\".format(\n",
    "                config, self.config_class\n",
    "            )\n",
    "        # initialize with config\n",
    "        super().__init__(config)\n",
    "\n",
    "        if encoder is None:\n",
    "            \n",
    "\n",
    "            encoder = AutoModel.from_config(config.encoder)\n",
    "\n",
    "        if decoder is None:\n",
    "            \n",
    "\n",
    "            decoder = AutoModelForCausalLM.from_config(config.decoder)\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.encoder._name_or_path)\n",
    "            \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        assert (\n",
    "            self.encoder.get_output_embeddings() is None\n",
    "        ), \"The encoder {} should not have a LM Head. Please use a model without LM Head\"\n",
    "\n",
    "        # tie encoder, decoder weights if config set accordingly\n",
    "        self.tie_weights()\n",
    "        \n",
    "        #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.embedding = self.encoder.embeddings\n",
    "        \n",
    "        self.mask_gate = nn.Linear(config.encoder.hidden_size, 1, bias=False)\n",
    "        self.copy_gate = nn.Linear(config.decoder.hidden_size, 1, bias=False)\n",
    "        self.generate_gate = nn.Linear(config.decoder.hidden_size, 1, bias=False)\n",
    "        self.skip_gate = nn.Linear(config.decoder.hidden_size, 1, bias=False)\n",
    "        \n",
    "        \n",
    "    def tie_weights(self):\n",
    "        # tie encoder & decoder if needed\n",
    "        if self.config.tie_encoder_decoder:\n",
    "            # tie encoder and decoder base model\n",
    "            decoder_base_model_prefix = self.decoder.base_model_prefix\n",
    "            self._tie_encoder_decoder_weights(\n",
    "                self.encoder, self.decoder._modules[decoder_base_model_prefix], self.decoder.base_model_prefix\n",
    "            )\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.encoder.get_input_embeddings()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.decoder.get_output_embeddings()\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        return self.decoder.set_output_embeddings(new_embeddings)\n",
    "\n",
    "    @classmethod\n",
    "    def from_encoder_decoder_pretrained(\n",
    "        cls,\n",
    "        encoder_pretrained_model_name_or_path: str = None,\n",
    "        decoder_pretrained_model_name_or_path: str = None,\n",
    "        *model_args,\n",
    "        **kwargs\n",
    "    ) -> PreTrainedModel:\n",
    "        r\"\"\"\n",
    "        Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model\n",
    "        checkpoints.\n",
    "        The model is set in evaluation mode by default using :obj:`model.eval()` (Dropout modules are deactivated). To\n",
    "        train the model, you need to first set it back in training mode with :obj:`model.train()`.\n",
    "        Params:\n",
    "            encoder_pretrained_model_name_or_path (:obj: `str`, `optional`):\n",
    "                Information necessary to initiate the encoder. Can be either:\n",
    "                    - A string, the `model id` of a pretrained model hosted inside a model repo on huggingface.co.\n",
    "                      Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under\n",
    "                      a user or organization name, like ``dbmdz/bert-base-german-cased``.\n",
    "                    - A path to a `directory` containing model weights saved using\n",
    "                      :func:`~transformers.PreTrainedModel.save_pretrained`, e.g., ``./my_model_directory/``.\n",
    "                    - A path or url to a `tensorflow index checkpoint file` (e.g, ``./tf_model/model.ckpt.index``). In\n",
    "                      this case, ``from_tf`` should be set to :obj:`True` and a configuration object should be provided\n",
    "                      as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in\n",
    "                      a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
    "            decoder_pretrained_model_name_or_path (:obj: `str`, `optional`, defaults to `None`):\n",
    "                Information necessary to initiate the decoder. Can be either:\n",
    "                    - A string, the `model id` of a pretrained model hosted inside a model repo on huggingface.co.\n",
    "                      Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under\n",
    "                      a user or organization name, like ``dbmdz/bert-base-german-cased``.\n",
    "                    - A path to a `directory` containing model weights saved using\n",
    "                      :func:`~transformers.PreTrainedModel.save_pretrained`, e.g., ``./my_model_directory/``.\n",
    "                    - A path or url to a `tensorflow index checkpoint file` (e.g, ``./tf_model/model.ckpt.index``). In\n",
    "                      this case, ``from_tf`` should be set to :obj:`True` and a configuration object should be provided\n",
    "                      as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in\n",
    "                      a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
    "            model_args (remaining positional arguments, `optional`):\n",
    "                All remaning positional arguments will be passed to the underlying model's ``__init__`` method.\n",
    "            kwargs (remaining dictionary of keyword arguments, `optional`):\n",
    "                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
    "                :obj:`output_attentions=True`).\n",
    "                - To update the encoder configuration, use the prefix `encoder_` for each configuration parameter.\n",
    "                - To update the decoder configuration, use the prefix `decoder_` for each configuration parameter.\n",
    "                - To update the parent model configuration, do not use a prefix for each configuration parameter.\n",
    "                Behaves differently depending on whether a :obj:`config` is provided or automatically loaded.\n",
    "        Example::\n",
    "            >>> from transformers import EncoderDecoderModel\n",
    "            >>> # initialize a bert2bert from two pretrained BERT models. Note that the cross-attention layers will be randomly initialized\n",
    "            >>> model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased')\n",
    "            >>> # saving model after fine-tuning\n",
    "            >>> model.save_pretrained(\"./bert2bert\")\n",
    "            >>> # load fine-tuned model\n",
    "            >>> model = EncoderDecoderModel.from_pretrained(\"./bert2bert\")\n",
    "        \"\"\"\n",
    "\n",
    "        kwargs_encoder = {\n",
    "            argument[len(\"encoder_\") :]: value for argument, value in kwargs.items() if argument.startswith(\"encoder_\")\n",
    "        }\n",
    "\n",
    "        kwargs_decoder = {\n",
    "            argument[len(\"decoder_\") :]: value for argument, value in kwargs.items() if argument.startswith(\"decoder_\")\n",
    "        }\n",
    "\n",
    "        # remove encoder, decoder kwargs from kwargs\n",
    "        for key in kwargs_encoder.keys():\n",
    "            del kwargs[\"encoder_\" + key]\n",
    "        for key in kwargs_decoder.keys():\n",
    "            del kwargs[\"decoder_\" + key]\n",
    "\n",
    "        # Load and initialize the encoder and decoder\n",
    "        # The distinction between encoder and decoder at the model level is made\n",
    "        # by the value of the flag `is_decoder` that we need to set correctly.\n",
    "        encoder = kwargs_encoder.pop(\"model\", None)\n",
    "        if encoder is None:\n",
    "            assert (\n",
    "                encoder_pretrained_model_name_or_path is not None\n",
    "            ), \"If `model` is not defined as an argument, a `encoder_pretrained_model_name_or_path` has to be defined\"\n",
    "\n",
    "            if \"config\" not in kwargs_encoder:\n",
    "\n",
    "                encoder_config = AutoConfig.from_pretrained(encoder_pretrained_model_name_or_path)\n",
    "                if encoder_config.is_decoder is True or encoder_config.add_cross_attention is True:\n",
    "\n",
    "                    logger.info(\n",
    "                        f\"Initializing {encoder_pretrained_model_name_or_path} as a encoder model from a decoder model. Cross-attention and casual mask are disabled.\"\n",
    "                    )\n",
    "                    encoder_config.is_decoder = False\n",
    "                    encoder_config.add_cross_attention = False\n",
    "\n",
    "                kwargs_encoder[\"config\"] = encoder_config\n",
    "\n",
    "            encoder = AutoModel.from_pretrained(encoder_pretrained_model_name_or_path, *model_args, **kwargs_encoder)\n",
    "\n",
    "        decoder = kwargs_decoder.pop(\"model\", None)\n",
    "        if decoder is None:\n",
    "            assert (\n",
    "                decoder_pretrained_model_name_or_path is not None\n",
    "            ), \"If `decoder_model` is not defined as an argument, a `decoder_pretrained_model_name_or_path` has to be defined\"\n",
    "\n",
    "            if \"config\" not in kwargs_decoder:\n",
    "\n",
    "                decoder_config = AutoConfig.from_pretrained(decoder_pretrained_model_name_or_path)\n",
    "                if decoder_config.is_decoder is False or decoder_config.add_cross_attention is False:\n",
    "                    logger.info(\n",
    "                        f\"Initializing {decoder_pretrained_model_name_or_path} as a decoder model. Cross attention layers are added to {decoder_pretrained_model_name_or_path} and randomly initialized if {decoder_pretrained_model_name_or_path}'s architecture allows for cross attention layers.\"\n",
    "                    )\n",
    "                    decoder_config.is_decoder = True\n",
    "                    decoder_config.add_cross_attention = True\n",
    "\n",
    "                kwargs_decoder[\"config\"] = decoder_config\n",
    "\n",
    "            if kwargs_decoder[\"config\"].is_decoder is False or kwargs_decoder[\"config\"].add_cross_attention is False:\n",
    "                logger.warning(\n",
    "                    f\"Decoder model {decoder_pretrained_model_name_or_path} is not initialized as a decoder. In order to initialize {decoder_pretrained_model_name_or_path} as a decoder, make sure that the attributes `is_decoder` and `add_cross_attention` of `decoder_config` passed to `.from_encoder_decoder_pretrained(...)` are set to `True` or do not pass a `decoder_config` to `.from_encoder_decoder_pretrained(...)`\"\n",
    "                )\n",
    "\n",
    "            decoder = AutoModelForCausalLM.from_pretrained(decoder_pretrained_model_name_or_path, **kwargs_decoder)\n",
    "\n",
    "        # instantiate config with corresponding kwargs\n",
    "        config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder.config, decoder.config, **kwargs)\n",
    "        return cls(encoder=encoder, decoder=decoder, config=config)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ENCODER_DECODER_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        encoder_mask_token_id,\n",
    "        decoder_mask_token_id,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,  # TODO: (PVP) implement :obj:`use_cache`\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,  # TODO: (PVP) implement :obj:`use_cache`\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "        Examples::\n",
    "            >>> from transformers import EncoderDecoderModel, BertTokenizer\n",
    "            >>> import torch\n",
    "            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            >>> model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased') # initialize Bert2Bert from pre-trained checkpoints\n",
    "            >>> # forward\n",
    "            >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "            >>> outputs = model(input_ids=input_ids, decoder_input_ids=input_ids)\n",
    "            >>> # training\n",
    "            >>> outputs = model(input_ids=input_ids, decoder_input_ids=input_ids, labels=input_ids)\n",
    "            >>> loss, logits = outputs.loss, outputs.logits\n",
    "            >>> # save and load from pretrained\n",
    "            >>> model.save_pretrained(\"bert2bert\")\n",
    "            >>> model = EncoderDecoderModel.from_pretrained(\"bert2bert\")\n",
    "            >>> # generation\n",
    "            >>> generated = model.generate(input_ids, decoder_start_token_id=model.config.decoder.pad_token_id)\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        kwargs_encoder = {argument: value for argument, value in kwargs.items() if not argument.startswith(\"decoder_\")}\n",
    "\n",
    "        kwargs_decoder = {\n",
    "            argument[len(\"decoder_\") :]: value for argument, value in kwargs.items() if argument.startswith(\"decoder_\")\n",
    "        }\n",
    "\n",
    "        if encoder_outputs is None:\n",
    "            encoder_outputs = self.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "                **kwargs_encoder,\n",
    "            )\n",
    "\n",
    "        mask_e = self.embedding(encoder_mask_token_id)\n",
    "        \n",
    "        encoder_hidden_states = encoder_outputs[0]\n",
    "        \n",
    "        masking_prob = F.sigmoid(self.mask_gate(encoder_hidden_states))\n",
    "        \n",
    "        encoder_hidden_states = masking_prob * mask_e + (1-masking_prob)*encoder_hidden_states\n",
    "        \n",
    "        #encoder_hidden_states = \n",
    "        \n",
    "        # Decode\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            labels=labels,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs_decoder,\n",
    "        )\n",
    "        \n",
    "        #print (decoder_outputs.logits.shape)\n",
    "        decoder_hidden_states = decoder_outputs.hidden_states[-1]\n",
    "        \n",
    "        copy_prob = F.sigmoid(self.copy_gate(decoder_hidden_states))\n",
    "        \n",
    "        generate_prob = F.sigmoid(self.generate_gate(decoder_hidden_states))\n",
    "        \n",
    "        #skip_prob = F.sigmoid(self.skip_gate(decoder_hidden_states))\n",
    "        \n",
    "        decoder_input_one_hot = torch.nn.functional.one_hot(decoder_input_ids, num_classes=self.config.decoder.vocab_size)\n",
    "        \n",
    "        stacks = nn.Softmax(dim=0)(torch.stack([copy_prob.unsqueeze(0),generate_prob.unsqueeze(0)]))\n",
    "        copy_prob = stacks[0]\n",
    "        generate_prob = stacks[1]\n",
    "        #skip_prob = stacks[2]\n",
    "        \n",
    "        #skip_logits = torch.zeros_like(decoder_outputs.logits)\n",
    "        #skip_logits[:,:,decoder_mask_token_id] = 1\n",
    "        \n",
    "        logits = generate_prob*decoder_outputs.logits + copy_prob*decoder_input_one_hot #+ skip_prob*skip_logits\n",
    "        #logits = decoder_outputs.logits\n",
    "        logits = nn.LogSoftmax(dim=-1)(logits)\n",
    "        \n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            \n",
    "            #loss_fn = nn.NLLLoss()\n",
    "            #loss = loss = loss_fn(logits.transpose(1, 2), labels)\n",
    "        else:\n",
    "            loss = None\n",
    "            \n",
    "        # TODO(PVP): currently it is not possible to use `past`\n",
    "        if not return_dict:\n",
    "            return decoder_outputs + encoder_outputs\n",
    "            #print (\"inside loop\")\n",
    "            #return (1-copy_prob)*decoder_outputs + copy_prob*encoder_outputs\n",
    "    \n",
    "        #print (decoder_outputs.hidden_states.shape)\n",
    "        #print (encoder_outputs.last_hidden_state.shape)\n",
    "        #print (encoder_outputs.hidden_states.shape)\n",
    "        \n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=None,  # TODO(PVP) - need to implement cache for BERT, etc... before this works\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        ), generate_prob, copy_prob,masking_prob  #, skip_prob\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, encoder_outputs=None, **kwargs):\n",
    "        decoder_inputs = self.decoder.prepare_inputs_for_generation(input_ids)\n",
    "        decoder_attention_mask = decoder_inputs[\"attention_mask\"] if \"attention_mask\" in decoder_inputs else None\n",
    "        input_dict = {\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"decoder_attention_mask\": decoder_attention_mask,\n",
    "            \"decoder_input_ids\": decoder_inputs[\"input_ids\"],\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "        }\n",
    "\n",
    "        # Ideally all models should have a :obj:`use_cache`\n",
    "        # leave following to ifs until all have it implemented\n",
    "        if \"use_cache\" in decoder_inputs:\n",
    "            input_dict[\"decoder_use_cache\"] = decoder_inputs[\"use_cache\"]\n",
    "\n",
    "        if \"past_key_values\" in decoder_inputs:\n",
    "            input_dict[\"past_key_values\"] = decoder_inputs[\"past_key_values\"]\n",
    "\n",
    "        return input_dict\n",
    "\n",
    "    def _reorder_cache(self, past, beam_idx):\n",
    "        # apply decoder cache reordering here\n",
    "        return self.decoder._reorder_cache(past, beam_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:29.551888Z",
     "iopub.status.busy": "2021-01-23T04:39:29.551232Z",
     "iopub.status.idle": "2021-01-23T04:39:53.577666Z",
     "shell.execute_reply": "2021-01-23T04:39:53.578230Z"
    },
    "papermill": {
     "duration": 24.16262,
     "end_time": "2021-01-23T04:39:53.578391",
     "exception": false,
     "start_time": "2021-01-23T04:39:29.415771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79eddc1e815049bd8bb77183a14cfbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoderModelWithGates.from_encoder_decoder_pretrained(encoder_path, decoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:53.868201Z",
     "iopub.status.busy": "2021-01-23T04:39:53.867565Z",
     "iopub.status.idle": "2021-01-23T04:39:53.872124Z",
     "shell.execute_reply": "2021-01-23T04:39:53.871560Z"
    },
    "papermill": {
     "duration": 0.146482,
     "end_time": "2021-01-23T04:39:53.872234",
     "exception": false,
     "start_time": "2021-01-23T04:39:53.725752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.encoder.max_length = MAX_LEN\n",
    "model.config.decoder.max_length = MAX_LEN\n",
    "\n",
    "model.config.encoder.min_length = MIN_LEN\n",
    "model.config.decoder.min_length = MIN_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:54.293545Z",
     "iopub.status.busy": "2021-01-23T04:39:54.292746Z",
     "iopub.status.idle": "2021-01-23T04:39:54.297714Z",
     "shell.execute_reply": "2021-01-23T04:39:54.298417Z"
    },
    "papermill": {
     "duration": 0.227025,
     "end_time": "2021-01-23T04:39:54.298607",
     "exception": false,
     "start_time": "2021-01-23T04:39:54.071582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and val loader length 1800 and 450\n"
     ]
    }
   ],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(trainX, trainy), batch_size=batch_size)\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(valX, valy), batch_size=batch_size)\n",
    "\n",
    "print (\"Train and val loader length {} and {}\".format(len(train_data_loader), len(val_data_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:39:54.600199Z",
     "iopub.status.busy": "2021-01-23T04:39:54.599433Z",
     "iopub.status.idle": "2021-01-23T04:40:00.899890Z",
     "shell.execute_reply": "2021-01-23T04:40:00.899127Z"
    },
    "papermill": {
     "duration": 6.458675,
     "end_time": "2021-01-23T04:40:00.900006",
     "exception": false,
     "start_time": "2021-01-23T04:39:54.441331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print (\"Modeling\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"Device: {}\".format(device))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = lr, # args.learning_rate\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T04:40:01.184584Z",
     "iopub.status.busy": "2021-01-23T04:40:01.183540Z",
     "iopub.status.idle": "2021-01-23T11:37:26.988931Z",
     "shell.execute_reply": "2021-01-23T11:37:26.989847Z"
    },
    "papermill": {
     "duration": 25045.953283,
     "end_time": "2021-01-23T11:37:26.990059",
     "exception": false,
     "start_time": "2021-01-23T04:40:01.036776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    60  of  1,800.    Elapsed: 0:00:38.  Loss: 3.857940435409546\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:15.  Loss: 2.994608163833618\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:53.  Loss: 3.5067882537841797\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:30.  Loss: 3.5969440937042236\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:08.  Loss: 3.5442821979522705\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:44.  Loss: 3.2522263526916504\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:22.  Loss: 2.135383367538452\n",
      "  Batch   480  of  1,800.    Elapsed: 0:04:59.  Loss: 2.324059247970581\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:36.  Loss: 3.6006722450256348\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:14.  Loss: 3.130993127822876\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:51.  Loss: 2.6174240112304688\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:28.  Loss: 3.6255788803100586\n",
      "  Batch   780  of  1,800.    Elapsed: 0:08:05.  Loss: 2.4201242923736572\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:42.  Loss: 2.7179245948791504\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:19.  Loss: 2.3198699951171875\n",
      "  Batch   960  of  1,800.    Elapsed: 0:09:57.  Loss: 2.349663496017456\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:34.  Loss: 2.615102767944336\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:11:11.  Loss: 1.8361623287200928\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:48.  Loss: 2.8731863498687744\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:26.  Loss: 1.9492403268814087\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:13:03.  Loss: 3.1494343280792236\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:40.  Loss: 3.62709379196167\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:14:18.  Loss: 2.5017919540405273\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:14:56.  Loss: 2.1998627185821533\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:33.  Loss: 3.6358869075775146\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:16:11.  Loss: 2.2556722164154053\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:48.  Loss: 4.1964311599731445\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:26.  Loss: 2.6397783756256104\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:18:04.  Loss: 3.3501181602478027\n",
      "\n",
      "  Average training loss: 2.83\n",
      "  Training epcoh took: 0:18:41\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 2.45\n",
      "  Validation took: 0:02:59\n",
      "\n",
      "======== Epoch 2 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:38.  Loss: 2.7869977951049805\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:16.  Loss: 2.3486058712005615\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:54.  Loss: 2.725376605987549\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:32.  Loss: 2.8883793354034424\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:10.  Loss: 2.9175467491149902\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:47.  Loss: 2.587843894958496\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:25.  Loss: 1.6584036350250244\n",
      "  Batch   480  of  1,800.    Elapsed: 0:05:03.  Loss: 1.7498449087142944\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:40.  Loss: 2.8745827674865723\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:18.  Loss: 2.3224494457244873\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:55.  Loss: 1.8226226568222046\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:33.  Loss: 2.4020321369171143\n",
      "  Batch   780  of  1,800.    Elapsed: 0:08:10.  Loss: 1.3609548807144165\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:47.  Loss: 1.3719722032546997\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:25.  Loss: 0.883036732673645\n",
      "  Batch   960  of  1,800.    Elapsed: 0:10:02.  Loss: 0.8439809679985046\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:40.  Loss: 0.6326609253883362\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:11:17.  Loss: 0.42587459087371826\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:54.  Loss: 0.5879648923873901\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:32.  Loss: 0.4579846262931824\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:13:10.  Loss: 0.5102723836898804\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:47.  Loss: 0.543481707572937\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:14:25.  Loss: 0.4059622585773468\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:15:02.  Loss: 0.3437560796737671\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:40.  Loss: 0.5063231587409973\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:16:17.  Loss: 0.3099516034126282\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:55.  Loss: 0.4052901566028595\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:32.  Loss: 0.22850075364112854\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:18:10.  Loss: 0.41997337341308594\n",
      "\n",
      "  Average training loss: 1.30\n",
      "  Training epcoh took: 0:18:47\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.20\n",
      "  Validation took: 0:02:57\n",
      "\n",
      "======== Epoch 3 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:38.  Loss: 0.34499961137771606\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:16.  Loss: 0.2852477431297302\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:53.  Loss: 0.20920142531394958\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:31.  Loss: 0.21199992299079895\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:08.  Loss: 0.28715988993644714\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:46.  Loss: 0.194618359208107\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:24.  Loss: 0.15527963638305664\n",
      "  Batch   480  of  1,800.    Elapsed: 0:05:01.  Loss: 0.2120150923728943\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:39.  Loss: 0.2568415105342865\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:16.  Loss: 0.19448399543762207\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:54.  Loss: 0.20866049826145172\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:31.  Loss: 0.24546188116073608\n",
      "  Batch   780  of  1,800.    Elapsed: 0:08:09.  Loss: 0.24140247702598572\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:46.  Loss: 0.2255234718322754\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:24.  Loss: 0.1357550024986267\n",
      "  Batch   960  of  1,800.    Elapsed: 0:10:02.  Loss: 0.12977159023284912\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:40.  Loss: 0.1504049003124237\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:11:17.  Loss: 0.12936177849769592\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:55.  Loss: 0.16651557385921478\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:33.  Loss: 0.16639135777950287\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:13:10.  Loss: 0.1722605973482132\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:47.  Loss: 0.20008082687854767\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:14:25.  Loss: 0.1365073025226593\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:15:02.  Loss: 0.11114810407161713\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:39.  Loss: 0.22436746954917908\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:16:16.  Loss: 0.12527291476726532\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:54.  Loss: 0.2245580106973648\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:31.  Loss: 0.09598842263221741\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:18:09.  Loss: 0.18408656120300293\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epcoh took: 0:18:46\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.14\n",
      "  Validation took: 0:02:54\n",
      "\n",
      "======== Epoch 4 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:38.  Loss: 0.19855189323425293\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:15.  Loss: 0.14666496217250824\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:52.  Loss: 0.11757399886846542\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:29.  Loss: 0.117821104824543\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:07.  Loss: 0.18996165692806244\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:45.  Loss: 0.1057538092136383\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:22.  Loss: 0.06758739054203033\n",
      "  Batch   480  of  1,800.    Elapsed: 0:04:59.  Loss: 0.0965922474861145\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:36.  Loss: 0.12908725440502167\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:13.  Loss: 0.09670545160770416\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:51.  Loss: 0.12330909818410873\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:28.  Loss: 0.15451441705226898\n",
      "  Batch   780  of  1,800.    Elapsed: 0:08:05.  Loss: 0.13651061058044434\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:43.  Loss: 0.139348104596138\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:20.  Loss: 0.061059460043907166\n",
      "  Batch   960  of  1,800.    Elapsed: 0:09:57.  Loss: 0.058142803609371185\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:34.  Loss: 0.0669589638710022\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:11:12.  Loss: 0.07204622775316238\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:50.  Loss: 0.10473597794771194\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:27.  Loss: 0.09791014343500137\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:13:04.  Loss: 0.09028436988592148\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:43.  Loss: 0.14578934013843536\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:14:20.  Loss: 0.09836408495903015\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:14:57.  Loss: 0.05733305960893631\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:35.  Loss: 0.1483483761548996\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:16:12.  Loss: 0.08521649986505508\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:50.  Loss: 0.15710735321044922\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:27.  Loss: 0.0556752011179924\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:18:04.  Loss: 0.12140113115310669\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epcoh took: 0:18:41\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.12\n",
      "  Validation took: 0:02:55\n",
      "\n",
      "======== Epoch 5 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:37.  Loss: 0.13961108028888702\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:15.  Loss: 0.1247931569814682\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:52.  Loss: 0.07780814915895462\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:29.  Loss: 0.06929078698158264\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:06.  Loss: 0.13389213383197784\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:43.  Loss: 0.06850355863571167\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:21.  Loss: 0.03397722542285919\n",
      "  Batch   480  of  1,800.    Elapsed: 0:04:58.  Loss: 0.0575658455491066\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:36.  Loss: 0.08923611044883728\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:13.  Loss: 0.057412419468164444\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:50.  Loss: 0.06972512602806091\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:27.  Loss: 0.12097584456205368\n",
      "  Batch   780  of  1,800.    Elapsed: 0:08:05.  Loss: 0.10108597576618195\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:42.  Loss: 0.09480839967727661\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:20.  Loss: 0.05347801744937897\n",
      "  Batch   960  of  1,800.    Elapsed: 0:09:57.  Loss: 0.04607955366373062\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:34.  Loss: 0.04934275522828102\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:11:12.  Loss: 0.040972720831632614\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:49.  Loss: 0.05974249169230461\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:26.  Loss: 0.05318750813603401\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:13:04.  Loss: 0.06053081899881363\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:41.  Loss: 0.10232948511838913\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:14:18.  Loss: 0.0550127848982811\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:14:56.  Loss: 0.04090869426727295\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:33.  Loss: 0.11357101052999496\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:16:10.  Loss: 0.04825114458799362\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:48.  Loss: 0.10924934595823288\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:25.  Loss: 0.03873942419886589\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:18:02.  Loss: 0.09298765659332275\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epcoh took: 0:18:39\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.11\n",
      "  Validation took: 0:02:58\n",
      "\n",
      "======== Epoch 6 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:38.  Loss: 0.10865002870559692\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:15.  Loss: 0.08097457885742188\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:52.  Loss: 0.044564712792634964\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:29.  Loss: 0.04098499193787575\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:06.  Loss: 0.09573964774608612\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:44.  Loss: 0.03391597047448158\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:21.  Loss: 0.031126143410801888\n",
      "  Batch   480  of  1,800.    Elapsed: 0:04:58.  Loss: 0.04641804099082947\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:36.  Loss: 0.06654729694128036\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:13.  Loss: 0.03715111315250397\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:50.  Loss: 0.0612744502723217\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:28.  Loss: 0.08192015439271927\n",
      "  Batch   780  of  1,800.    Elapsed: 0:08:05.  Loss: 0.05153965204954147\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:43.  Loss: 0.05878203734755516\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:20.  Loss: 0.03591097891330719\n",
      "  Batch   960  of  1,800.    Elapsed: 0:09:57.  Loss: 0.02143232896924019\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:35.  Loss: 0.028739087283611298\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:11:11.  Loss: 0.020564591512084007\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:49.  Loss: 0.030548594892024994\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:27.  Loss: 0.028765402734279633\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:13:03.  Loss: 0.052581992000341415\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:41.  Loss: 0.05536479130387306\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:14:18.  Loss: 0.04396083578467369\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:14:55.  Loss: 0.019955921918153763\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:31.  Loss: 0.06999622285366058\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:16:08.  Loss: 0.027659283950924873\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:45.  Loss: 0.08552046120166779\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:22.  Loss: 0.022909948602318764\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:17:59.  Loss: 0.056120071560144424\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epcoh took: 0:18:36\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.11\n",
      "  Validation took: 0:02:56\n",
      "\n",
      "======== Epoch 7 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:37.  Loss: 0.07132504135370255\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:14.  Loss: 0.05502410605549812\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:52.  Loss: 0.028949666768312454\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:28.  Loss: 0.037125371396541595\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:05.  Loss: 0.06508730351924896\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:42.  Loss: 0.025595275685191154\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:19.  Loss: 0.015241092070937157\n",
      "  Batch   480  of  1,800.    Elapsed: 0:04:56.  Loss: 0.015236246399581432\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:33.  Loss: 0.04356473311781883\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:10.  Loss: 0.027089618146419525\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:46.  Loss: 0.030009638518095016\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:23.  Loss: 0.04918982833623886\n",
      "  Batch   780  of  1,800.    Elapsed: 0:08:00.  Loss: 0.04349886253476143\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:36.  Loss: 0.04247980937361717\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:13.  Loss: 0.026695240288972855\n",
      "  Batch   960  of  1,800.    Elapsed: 0:09:50.  Loss: 0.012626320123672485\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:26.  Loss: 0.01671513170003891\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:11:03.  Loss: 0.014634305611252785\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:40.  Loss: 0.024654993787407875\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:17.  Loss: 0.016154054552316666\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:12:53.  Loss: 0.029826439917087555\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:30.  Loss: 0.04855842515826225\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:14:07.  Loss: 0.03746828809380531\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:14:44.  Loss: 0.011129878461360931\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:20.  Loss: 0.06349245458841324\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:15:57.  Loss: 0.010846847668290138\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:34.  Loss: 0.05043817684054375\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:11.  Loss: 0.013851874507963657\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:17:48.  Loss: 0.03928759694099426\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epcoh took: 0:18:25\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.10\n",
      "  Validation took: 0:02:57\n",
      "\n",
      "======== Epoch 8 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:37.  Loss: 0.04992470145225525\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:14.  Loss: 0.040474191308021545\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:51.  Loss: 0.019184598699212074\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:28.  Loss: 0.023455070331692696\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:04.  Loss: 0.05383351445198059\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:41.  Loss: 0.02574651688337326\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:18.  Loss: 0.014462484046816826\n",
      "  Batch   480  of  1,800.    Elapsed: 0:04:54.  Loss: 0.011267339810729027\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:31.  Loss: 0.03882680460810661\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:08.  Loss: 0.01834738813340664\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:45.  Loss: 0.015217489562928677\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:21.  Loss: 0.04137983173131943\n",
      "  Batch   780  of  1,800.    Elapsed: 0:07:58.  Loss: 0.018950289115309715\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:34.  Loss: 0.030310772359371185\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:11.  Loss: 0.010960402898490429\n",
      "  Batch   960  of  1,800.    Elapsed: 0:09:48.  Loss: 0.008723381906747818\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:25.  Loss: 0.016707461327314377\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:11:02.  Loss: 0.009113507345318794\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:39.  Loss: 0.009089146740734577\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:15.  Loss: 0.01450352929532528\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:12:52.  Loss: 0.015307288616895676\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:29.  Loss: 0.026567230001091957\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:14:06.  Loss: 0.016843117773532867\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:14:42.  Loss: 0.007124987430870533\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:19.  Loss: 0.029409948736429214\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:15:56.  Loss: 0.005546391010284424\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:32.  Loss: 0.04146035388112068\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:09.  Loss: 0.009528025053441525\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:17:46.  Loss: 0.03197784349322319\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epcoh took: 0:18:22\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.10\n",
      "  Validation took: 0:02:53\n",
      "\n",
      "======== Epoch 9 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:37.  Loss: 0.03301936760544777\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:14.  Loss: 0.027356646955013275\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:51.  Loss: 0.014786831103265285\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:27.  Loss: 0.020210575312376022\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:05.  Loss: 0.0346079058945179\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:41.  Loss: 0.02209007740020752\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:18.  Loss: 0.011371958069503307\n",
      "  Batch   480  of  1,800.    Elapsed: 0:04:55.  Loss: 0.009916108101606369\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:32.  Loss: 0.023711733520030975\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:09.  Loss: 0.016766196116805077\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:46.  Loss: 0.019984448328614235\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:23.  Loss: 0.04330082982778549\n",
      "  Batch   780  of  1,800.    Elapsed: 0:08:00.  Loss: 0.014301602728664875\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:37.  Loss: 0.023206369951367378\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:14.  Loss: 0.016995055601000786\n",
      "  Batch   960  of  1,800.    Elapsed: 0:09:51.  Loss: 0.010294816456735134\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:28.  Loss: 0.015132660046219826\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:11:05.  Loss: 0.007345465011894703\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:41.  Loss: 0.007491604890674353\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:18.  Loss: 0.016272179782390594\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:12:55.  Loss: 0.016261951997876167\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:32.  Loss: 0.017408674582839012\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:14:09.  Loss: 0.01789672113955021\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:14:45.  Loss: 0.012128835543990135\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:22.  Loss: 0.038147397339344025\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:15:59.  Loss: 0.00647481856867671\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:36.  Loss: 0.02670430578291416\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:13.  Loss: 0.00806177593767643\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:17:49.  Loss: 0.024286244064569473\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epcoh took: 0:18:26\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.10\n",
      "  Validation took: 0:02:05\n",
      "\n",
      "======== Epoch 10 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:37.  Loss: 0.019224461168050766\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:13.  Loss: 0.019250944256782532\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:50.  Loss: 0.010260319337248802\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:27.  Loss: 0.013571461662650108\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:04.  Loss: 0.01854080893099308\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:41.  Loss: 0.007709792349487543\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:17.  Loss: 0.004547666292637587\n",
      "  Batch   480  of  1,800.    Elapsed: 0:04:54.  Loss: 0.0032148088794201612\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:30.  Loss: 0.013107468374073505\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:07.  Loss: 0.010241743177175522\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:44.  Loss: 0.011635021306574345\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:20.  Loss: 0.017928127199411392\n",
      "  Batch   780  of  1,800.    Elapsed: 0:07:57.  Loss: 0.013724792748689651\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:34.  Loss: 0.015166948549449444\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:10.  Loss: 0.0054992553777992725\n",
      "  Batch   960  of  1,800.    Elapsed: 0:09:47.  Loss: 0.008786464110016823\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:23.  Loss: 0.0070939743891358376\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:11:00.  Loss: 0.005740093532949686\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:37.  Loss: 0.00700426334515214\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:14.  Loss: 0.004270253702998161\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:12:51.  Loss: 0.009222330525517464\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:27.  Loss: 0.01814957708120346\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:14:04.  Loss: 0.009331178851425648\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:14:40.  Loss: 0.003651890205219388\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:16.  Loss: 0.02631346881389618\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:15:53.  Loss: 0.005269051995128393\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:30.  Loss: 0.01624392718076706\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:06.  Loss: 0.002942441962659359\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:17:43.  Loss: 0.02103361301124096\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:18:19\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.10\n",
      "  Validation took: 0:02:04\n",
      "\n",
      "======== Epoch 11 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:36.  Loss: 0.02062670700252056\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:13.  Loss: 0.013114427216351032\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:49.  Loss: 0.012909428216516972\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:25.  Loss: 0.009822524152696133\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:02.  Loss: 0.015059921890497208\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:39.  Loss: 0.005168466828763485\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:15.  Loss: 0.002427296945825219\n",
      "  Batch   480  of  1,800.    Elapsed: 0:04:51.  Loss: 0.0035556431394070387\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:28.  Loss: 0.0036463167052716017\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:05.  Loss: 0.00406139250844717\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:41.  Loss: 0.010899970307946205\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:18.  Loss: 0.01447206549346447\n",
      "  Batch   780  of  1,800.    Elapsed: 0:07:55.  Loss: 0.005691187921911478\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:31.  Loss: 0.007546932902187109\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:08.  Loss: 0.002505379030480981\n",
      "  Batch   960  of  1,800.    Elapsed: 0:09:45.  Loss: 0.003402512287721038\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:21.  Loss: 0.008791105821728706\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:10:58.  Loss: 0.0015233864542096853\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:35.  Loss: 0.005830081645399332\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:12.  Loss: 0.013878850266337395\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:12:48.  Loss: 0.006715843919664621\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:25.  Loss: 0.017105499282479286\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:14:01.  Loss: 0.0065718479454517365\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:14:38.  Loss: 0.0036982730962336063\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:15.  Loss: 0.02622651867568493\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:15:51.  Loss: 0.002509145764634013\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:28.  Loss: 0.00922125019133091\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:05.  Loss: 0.006919581908732653\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:17:41.  Loss: 0.011671248823404312\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:18:18\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.10\n",
      "  Validation took: 0:02:04\n",
      "\n",
      "======== Epoch 12 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:36.  Loss: 0.008408933877944946\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:13.  Loss: 0.005408977624028921\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:50.  Loss: 0.005977170076221228\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:26.  Loss: 0.011096245609223843\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:02.  Loss: 0.01235103514045477\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:39.  Loss: 0.01557133812457323\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:15.  Loss: 0.005687393713742495\n",
      "  Batch   480  of  1,800.    Elapsed: 0:04:52.  Loss: 0.003735268022865057\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:28.  Loss: 0.00622788118198514\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:05.  Loss: 0.006051025353372097\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:41.  Loss: 0.0044007692486047745\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:18.  Loss: 0.005949554033577442\n",
      "  Batch   780  of  1,800.    Elapsed: 0:07:55.  Loss: 0.017739318311214447\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:31.  Loss: 0.0039161513559520245\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:08.  Loss: 0.005877304822206497\n",
      "  Batch   960  of  1,800.    Elapsed: 0:09:44.  Loss: 0.003631497733294964\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:20.  Loss: 0.006187228485941887\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:10:57.  Loss: 0.0027548749931156635\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:34.  Loss: 0.014717484824359417\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:11.  Loss: 0.00288715329952538\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:12:48.  Loss: 0.0066746738739311695\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:24.  Loss: 0.00605629850178957\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:14:01.  Loss: 0.01810140535235405\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:14:38.  Loss: 0.003011827589944005\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:14.  Loss: 0.014775094576179981\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:15:51.  Loss: 0.002129666041582823\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:28.  Loss: 0.003100644564256072\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:05.  Loss: 0.0021129760425537825\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:17:41.  Loss: 0.013878162950277328\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:18:18\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.10\n",
      "  Validation took: 0:02:05\n",
      "\n",
      "======== Epoch 13 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:37.  Loss: 0.01499136071652174\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:14.  Loss: 0.0034224025439471006\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:50.  Loss: 0.009882970713078976\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:27.  Loss: 0.011973866261541843\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:04.  Loss: 0.006351241376250982\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:40.  Loss: 0.0035706304479390383\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:17.  Loss: 0.0011144067393615842\n",
      "  Batch   480  of  1,800.    Elapsed: 0:04:53.  Loss: 0.0017764008371159434\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:30.  Loss: 0.00783799309283495\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:06.  Loss: 0.004606931935995817\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:43.  Loss: 0.0065230028703808784\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:20.  Loss: 0.004155649803578854\n",
      "  Batch   780  of  1,800.    Elapsed: 0:07:56.  Loss: 0.0030714021995663643\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:33.  Loss: 0.007406173273921013\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:09.  Loss: 0.008412960916757584\n",
      "  Batch   960  of  1,800.    Elapsed: 0:09:46.  Loss: 0.003575521055608988\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:23.  Loss: 0.004401250742375851\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:10:59.  Loss: 0.005971554201096296\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:36.  Loss: 0.009996294975280762\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:13.  Loss: 0.004690960049629211\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:12:50.  Loss: 0.009189301170408726\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:27.  Loss: 0.004185352940112352\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:14:03.  Loss: 0.0032306108623743057\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:14:39.  Loss: 0.007538971956819296\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:16.  Loss: 0.008179128170013428\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:15:52.  Loss: 0.0014339451445266604\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:29.  Loss: 0.005172759760171175\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:06.  Loss: 0.0012684202520176768\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:17:42.  Loss: 0.0051236022263765335\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:18:19\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.10\n",
      "  Validation took: 0:02:04\n",
      "\n",
      "======== Epoch 14 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:36.  Loss: 0.004525387659668922\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:13.  Loss: 0.003722608555108309\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:49.  Loss: 0.0018354138592258096\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:26.  Loss: 0.001873502740636468\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:02.  Loss: 0.005172784440219402\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:39.  Loss: 0.0029398854821920395\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:15.  Loss: 0.0019941432401537895\n",
      "  Batch   480  of  1,800.    Elapsed: 0:04:52.  Loss: 0.002228944795206189\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:29.  Loss: 0.0036807511933147907\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:05.  Loss: 0.0022056209854781628\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:41.  Loss: 0.0060491133481264114\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:18.  Loss: 0.0024321251548826694\n",
      "  Batch   780  of  1,800.    Elapsed: 0:07:55.  Loss: 0.006878749933093786\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:31.  Loss: 0.005619730334728956\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:08.  Loss: 0.0014144902816042304\n",
      "  Batch   960  of  1,800.    Elapsed: 0:09:44.  Loss: 0.0023116699885576963\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:20.  Loss: 0.0023927674628794193\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:10:57.  Loss: 0.0023851424921303988\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:33.  Loss: 0.004747719969600439\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:10.  Loss: 0.0022585538681596518\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:12:47.  Loss: 0.01903717778623104\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:23.  Loss: 0.007460463792085648\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:14:00.  Loss: 0.002440352225676179\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:14:36.  Loss: 0.0018832226051017642\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:12.  Loss: 0.01047526579350233\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:15:49.  Loss: 0.005191929172724485\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:26.  Loss: 0.00507989339530468\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:02.  Loss: 0.0017442975658923388\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:17:38.  Loss: 0.006181269884109497\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:18:15\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.10\n",
      "  Validation took: 0:02:05\n",
      "\n",
      "======== Epoch 15 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:37.  Loss: 0.0030403323471546173\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:13.  Loss: 0.0026189477648586035\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:50.  Loss: 0.0011377460323274136\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:26.  Loss: 0.0018510380759835243\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:02.  Loss: 0.007473391480743885\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:39.  Loss: 0.008645261637866497\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:15.  Loss: 0.0016258968971669674\n",
      "  Batch   480  of  1,800.    Elapsed: 0:04:52.  Loss: 0.008366920985281467\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:28.  Loss: 0.012212391942739487\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:05.  Loss: 0.002111696405336261\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:42.  Loss: 0.002286445815116167\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:18.  Loss: 0.004260652232915163\n",
      "  Batch   780  of  1,800.    Elapsed: 0:07:55.  Loss: 0.0010775953996926546\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:31.  Loss: 0.004880635067820549\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:07.  Loss: 0.0006763976998627186\n",
      "  Batch   960  of  1,800.    Elapsed: 0:09:44.  Loss: 0.0023183429148048162\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:20.  Loss: 0.0008873221813701093\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:10:57.  Loss: 0.0014566336758434772\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:34.  Loss: 0.004045835696160793\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:10.  Loss: 0.0012819628464058042\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:12:47.  Loss: 0.002930273301899433\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:24.  Loss: 0.0034984941594302654\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:14:00.  Loss: 0.0019397655269131064\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:14:37.  Loss: 0.0008002941613085568\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:13.  Loss: 0.004426646512001753\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:15:50.  Loss: 0.0011820807121694088\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:26.  Loss: 0.0021603878121823072\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:03.  Loss: 0.0010351806413382292\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:17:40.  Loss: 0.007173978723585606\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:18:16\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.10\n",
      "  Validation took: 0:02:04\n",
      "\n",
      "======== Epoch 16 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:37.  Loss: 0.004281448200345039\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:14.  Loss: 0.0015091347740963101\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:50.  Loss: 0.003749614581465721\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:27.  Loss: 0.0017240470042452216\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:04.  Loss: 0.001523886458016932\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:41.  Loss: 0.001319470233283937\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:17.  Loss: 0.0007626730366609991\n",
      "  Batch   480  of  1,800.    Elapsed: 0:04:54.  Loss: 0.0010206764563918114\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:31.  Loss: 0.0033095949329435825\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:08.  Loss: 0.0010770813096314669\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:44.  Loss: 0.004556973930448294\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:21.  Loss: 0.001961813773959875\n",
      "  Batch   780  of  1,800.    Elapsed: 0:07:57.  Loss: 0.00239472440443933\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:34.  Loss: 0.0017708457307890058\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:11.  Loss: 0.0007416363223455846\n",
      "  Batch   960  of  1,800.    Elapsed: 0:09:47.  Loss: 0.0012087884824723005\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:24.  Loss: 0.0036330590955913067\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:11:00.  Loss: 0.0007678015390411019\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:37.  Loss: 0.0034931909758597612\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:14.  Loss: 0.0007817783043719828\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:12:50.  Loss: 0.0049159107729792595\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:27.  Loss: 0.003386662108823657\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:14:03.  Loss: 0.0013151520397514105\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:14:40.  Loss: 0.001126290881074965\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:17.  Loss: 0.0039051377680152655\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:15:53.  Loss: 0.0010390839306637645\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:30.  Loss: 0.0021992961410433054\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:06.  Loss: 0.0007501090876758099\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:17:42.  Loss: 0.0016694471705704927\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:18:19\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.10\n",
      "  Validation took: 0:02:04\n",
      "\n",
      "======== Epoch 17 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:36.  Loss: 0.0014131215866655111\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:13.  Loss: 0.0017333978321403265\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:49.  Loss: 0.0018554360140115023\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:26.  Loss: 0.0007934634340927005\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:02.  Loss: 0.0030571995303034782\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:39.  Loss: 0.0016712283249944448\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:15.  Loss: 0.0003841307188849896\n",
      "  Batch   480  of  1,800.    Elapsed: 0:04:52.  Loss: 0.005427428055554628\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:28.  Loss: 0.0010254636872559786\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:04.  Loss: 0.0008527761092409492\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:41.  Loss: 0.0019468643004074693\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:17.  Loss: 0.008503449149429798\n",
      "  Batch   780  of  1,800.    Elapsed: 0:07:54.  Loss: 0.010493898764252663\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:30.  Loss: 0.0028857956640422344\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:07.  Loss: 0.0008396876510232687\n",
      "  Batch   960  of  1,800.    Elapsed: 0:09:43.  Loss: 0.001742420019581914\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:20.  Loss: 0.0016535158501937985\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:10:56.  Loss: 0.003313760505989194\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:32.  Loss: 0.0013188027078285813\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:09.  Loss: 0.0009430992649868131\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:12:46.  Loss: 0.0017917745281010866\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:22.  Loss: 0.0033212474081665277\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:13:59.  Loss: 0.001986842602491379\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:14:36.  Loss: 0.00270284921862185\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:13.  Loss: 0.002251139609143138\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:15:50.  Loss: 0.002461853437125683\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:26.  Loss: 0.001989237731322646\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:02.  Loss: 0.0008602587622590363\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:17:39.  Loss: 0.0027036310639232397\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:18:16\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.10\n",
      "  Validation took: 0:02:03\n",
      "\n",
      "======== Epoch 18 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:36.  Loss: 0.0012573947897180915\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:13.  Loss: 0.0013209260068833828\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:49.  Loss: 0.0006772910710424185\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:26.  Loss: 0.001938643748871982\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:02.  Loss: 0.0012396466918289661\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:38.  Loss: 0.0015413487562909722\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:15.  Loss: 0.00040805269964039326\n",
      "  Batch   480  of  1,800.    Elapsed: 0:04:51.  Loss: 0.0010405940702185035\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:28.  Loss: 0.001440441352315247\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:04.  Loss: 0.0006318387459032238\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:40.  Loss: 0.0034078285098075867\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:17.  Loss: 0.003120607230812311\n",
      "  Batch   780  of  1,800.    Elapsed: 0:07:53.  Loss: 0.001087177312001586\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:30.  Loss: 0.001277090748772025\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:06.  Loss: 0.0005333709414117038\n",
      "  Batch   960  of  1,800.    Elapsed: 0:09:42.  Loss: 0.000756230961997062\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:19.  Loss: 0.002312160562723875\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:10:55.  Loss: 0.0008951989002525806\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:32.  Loss: 0.0007137877400964499\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:08.  Loss: 0.0006739192176610231\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:12:44.  Loss: 0.0007031162385828793\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:21.  Loss: 0.0012614496517926455\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:13:57.  Loss: 0.000646171742118895\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:14:34.  Loss: 0.000554318365175277\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:10.  Loss: 0.0018213638104498386\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:15:46.  Loss: 0.0004374465497676283\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:23.  Loss: 0.0008204833138734102\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:16:59.  Loss: 0.0005957503453828394\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:17:35.  Loss: 0.002300378866493702\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:18:11\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.10\n",
      "  Validation took: 0:02:04\n",
      "\n",
      "======== Epoch 19 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:37.  Loss: 0.0010929069248959422\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:13.  Loss: 0.0007384343189187348\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:49.  Loss: 0.0011962680146098137\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:25.  Loss: 0.0006967735826037824\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:02.  Loss: 0.000968828157056123\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:38.  Loss: 0.00118816620670259\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:15.  Loss: 0.0003245960979256779\n",
      "  Batch   480  of  1,800.    Elapsed: 0:04:51.  Loss: 0.0007686857134103775\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:28.  Loss: 0.000651789247058332\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:04.  Loss: 0.0005116603570058942\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:40.  Loss: 0.0011623840546235442\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:17.  Loss: 0.0008608249481767416\n",
      "  Batch   780  of  1,800.    Elapsed: 0:07:53.  Loss: 0.0018785232678055763\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:29.  Loss: 0.0008895943174138665\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:06.  Loss: 0.00040424399776384234\n",
      "  Batch   960  of  1,800.    Elapsed: 0:09:42.  Loss: 0.00040141813224181533\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:19.  Loss: 0.001072271610610187\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:10:55.  Loss: 0.0005133128142915666\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:31.  Loss: 0.0005032953922636807\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:08.  Loss: 0.0005239027668721974\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:12:44.  Loss: 0.004137682728469372\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:21.  Loss: 0.0017562340945005417\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:13:57.  Loss: 0.0006128213717602193\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:14:34.  Loss: 0.00044561721733771265\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:10.  Loss: 0.005607671104371548\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:15:47.  Loss: 0.0004008981632068753\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:24.  Loss: 0.0008468812447972596\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:01.  Loss: 0.0002826805575750768\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:17:37.  Loss: 0.004670781083405018\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:18:14\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.10\n",
      "  Validation took: 0:02:04\n",
      "\n",
      "======== Epoch 20 / 20 ========\n",
      "Training...\n",
      "  Batch    60  of  1,800.    Elapsed: 0:00:36.  Loss: 0.0010907340329140425\n",
      "  Batch   120  of  1,800.    Elapsed: 0:01:13.  Loss: 0.00038202156429179013\n",
      "  Batch   180  of  1,800.    Elapsed: 0:01:49.  Loss: 0.000940388476010412\n",
      "  Batch   240  of  1,800.    Elapsed: 0:02:25.  Loss: 0.00045356564805842936\n",
      "  Batch   300  of  1,800.    Elapsed: 0:03:02.  Loss: 0.001122740563005209\n",
      "  Batch   360  of  1,800.    Elapsed: 0:03:38.  Loss: 0.0010320110013708472\n",
      "  Batch   420  of  1,800.    Elapsed: 0:04:15.  Loss: 0.00036460228147916496\n",
      "  Batch   480  of  1,800.    Elapsed: 0:04:51.  Loss: 0.00033536553382873535\n",
      "  Batch   540  of  1,800.    Elapsed: 0:05:27.  Loss: 0.002445901744067669\n",
      "  Batch   600  of  1,800.    Elapsed: 0:06:04.  Loss: 0.00042859790846705437\n",
      "  Batch   660  of  1,800.    Elapsed: 0:06:40.  Loss: 0.0008934532525017858\n",
      "  Batch   720  of  1,800.    Elapsed: 0:07:17.  Loss: 0.0005111205973662436\n",
      "  Batch   780  of  1,800.    Elapsed: 0:07:53.  Loss: 0.0006715195486322045\n",
      "  Batch   840  of  1,800.    Elapsed: 0:08:30.  Loss: 0.0010154973715543747\n",
      "  Batch   900  of  1,800.    Elapsed: 0:09:06.  Loss: 0.0004914112505502999\n",
      "  Batch   960  of  1,800.    Elapsed: 0:09:43.  Loss: 0.00033911984064616263\n",
      "  Batch 1,020  of  1,800.    Elapsed: 0:10:19.  Loss: 0.0006542567280121148\n",
      "  Batch 1,080  of  1,800.    Elapsed: 0:10:55.  Loss: 0.0005237296572886407\n",
      "  Batch 1,140  of  1,800.    Elapsed: 0:11:32.  Loss: 0.001120509346947074\n",
      "  Batch 1,200  of  1,800.    Elapsed: 0:12:08.  Loss: 0.0007144008995965123\n",
      "  Batch 1,260  of  1,800.    Elapsed: 0:12:45.  Loss: 0.0034305835142731667\n",
      "  Batch 1,320  of  1,800.    Elapsed: 0:13:21.  Loss: 0.0019104254897683859\n",
      "  Batch 1,380  of  1,800.    Elapsed: 0:13:58.  Loss: 0.0005363909876905382\n",
      "  Batch 1,440  of  1,800.    Elapsed: 0:14:34.  Loss: 0.0005686025833711028\n",
      "  Batch 1,500  of  1,800.    Elapsed: 0:15:11.  Loss: 0.001026508747600019\n",
      "  Batch 1,560  of  1,800.    Elapsed: 0:15:48.  Loss: 0.0003639364440459758\n",
      "  Batch 1,620  of  1,800.    Elapsed: 0:16:25.  Loss: 0.0015668831765651703\n",
      "  Batch 1,680  of  1,800.    Elapsed: 0:17:01.  Loss: 0.00021036718680988997\n",
      "  Batch 1,740  of  1,800.    Elapsed: 0:17:38.  Loss: 0.0020067698787897825\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:18:14\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.10\n",
      "  Validation took: 0:02:04\n",
      "\n",
      "Training complete!\n",
      "Total training took 6:57:25 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "bad_epochs = 0\n",
    "best_val_logits = 0\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, EPOCHS):\n",
    "    \n",
    "    if bad_epochs < early_stopping_rounds:\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "\n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_data_loader):\n",
    "\n",
    "            # Progress update every 40 batches.\n",
    "            if step % 60 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.  Loss: {}'.format(step, len(train_data_loader), elapsed, loss.item()))\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "            # `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            input_ids = batch[0].to(device)\n",
    "            output_ids = batch[1].to(device)\n",
    "\n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass. PyTorch doesn't do this automatically because \n",
    "            # accumulating the gradients is \"convenient while training RNNs\". \n",
    "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "            model.zero_grad()        \n",
    "\n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # It returns different numbers of parameters depending on what arguments\n",
    "            # are given and what flags are set. For our usage here, it returns\n",
    "            # the loss (because we provided labels) and the \"logits\"--the model\n",
    "            # outputs prior to activation.\n",
    "            if teacher_forcing:\n",
    "                outputs, _, _, _ = model(encoder_mask_token_id = torch.tensor([[encoder_tokenizer.mask_token_id]]).to(device),\\\n",
    "                                     decoder_mask_token_id = decoder_tokenizer.mask_token_id, \\\n",
    "                            input_ids=input_ids, decoder_input_ids=output_ids, labels=output_ids, return_dict=True)\n",
    "            else:\n",
    "                outputs, _, _, _ = model(encoder_mask_token_id = torch.tensor([[encoder_tokenizer.mask_token_id]]).to(device),\\\n",
    "                                     decoder_mask_token_id = decoder_tokenizer.mask_token_id, \\\n",
    "                            input_ids=input_ids, decoder_input_ids=input_ids, labels=output_ids, return_dict=True)\n",
    "                \n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_data_loader)            \n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        all_val_logits = []\n",
    "        \n",
    "        # Evaluate data for one epoch\n",
    "        for batch in val_data_loader:\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "            # the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            input_ids = batch[0].to(device)\n",
    "            output_ids = batch[1].to(device)\n",
    "\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "                if teacher_forcing:\n",
    "                    outputs, _, _, _ = model(encoder_mask_token_id = torch.tensor([[encoder_tokenizer.mask_token_id]]).to(device),\\\n",
    "                                     decoder_mask_token_id = decoder_tokenizer.mask_token_id, \\\n",
    "                            input_ids=input_ids, decoder_input_ids=output_ids, labels=output_ids, return_dict=True)\n",
    "                else:\n",
    "                    outputs, _, _, _ = model(encoder_mask_token_id = torch.tensor([[encoder_tokenizer.mask_token_id]]).to(device),\\\n",
    "                                     decoder_mask_token_id = decoder_tokenizer.mask_token_id, \\\n",
    "                            input_ids=input_ids, decoder_input_ids=input_ids, labels=output_ids, return_dict=True)\n",
    "                \n",
    "                loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "\n",
    "            # Accumulate the validation loss.\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            all_val_logits.extend(logits.argmax(-1))\n",
    "            \n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(val_data_loader)\n",
    "\n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        if epoch_i >= 1:\n",
    "            if avg_val_loss < training_stats[-1]['Valid. Loss']:\n",
    "                #model.save_pretrained('Bert2Bert_denoise')\n",
    "                torch.save(model.state_dict(), 'gated_Bert2Bert_denoise.bin')\n",
    "                bad_epochs = 0\n",
    "                best_val_logits = all_val_logits.copy()\n",
    "            else:\n",
    "                bad_epochs += 1\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        print (\"Early stopping!!\")\n",
    "        break\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T11:37:27.742184Z",
     "iopub.status.busy": "2021-01-23T11:37:27.741031Z",
     "iopub.status.idle": "2021-01-23T11:37:27.743029Z",
     "shell.execute_reply": "2021-01-23T11:37:27.743623Z"
    },
    "papermill": {
     "duration": 0.394115,
     "end_time": "2021-01-23T11:37:27.743768",
     "exception": false,
     "start_time": "2021-01-23T11:37:27.349653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('BERT_denoise.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T11:37:28.498328Z",
     "iopub.status.busy": "2021-01-23T11:37:28.497562Z",
     "iopub.status.idle": "2021-01-23T11:37:28.501156Z",
     "shell.execute_reply": "2021-01-23T11:37:28.500589Z"
    },
    "papermill": {
     "duration": 0.382562,
     "end_time": "2021-01-23T11:37:28.501274",
     "exception": false,
     "start_time": "2021-01-23T11:37:28.118712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('training_stats.txt','w') as f:\n",
    "    for l in training_stats:\n",
    "        f.write(str(l))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T11:37:29.259844Z",
     "iopub.status.busy": "2021-01-23T11:37:29.258775Z",
     "iopub.status.idle": "2021-01-23T11:37:29.437840Z",
     "shell.execute_reply": "2021-01-23T11:37:29.437205Z"
    },
    "papermill": {
     "duration": 0.5613,
     "end_time": "2021-01-23T11:37:29.437974",
     "exception": false,
     "start_time": "2021-01-23T11:37:28.876674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch in val_data_loader:\n",
    "    input_ids = batch[0].to(device)\n",
    "    output_ids = batch[1].to(device)\n",
    "    break\n",
    "\n",
    "with torch.no_grad():        \n",
    "    outputs, generate_prob, copy_prob, mask_prob = model(encoder_mask_token_id = torch.tensor([[encoder_tokenizer.mask_token_id]]).to(device), \\\n",
    "                                                            decoder_mask_token_id = decoder_tokenizer.mask_token_id, \\\n",
    "                    input_ids=input_ids, decoder_input_ids=input_ids, labels=output_ids, return_dict=True)\n",
    "    loss, logits = outputs.loss, outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T11:37:30.212432Z",
     "iopub.status.busy": "2021-01-23T11:37:30.211451Z",
     "iopub.status.idle": "2021-01-23T11:37:30.322226Z",
     "shell.execute_reply": "2021-01-23T11:37:30.321676Z"
    },
    "papermill": {
     "duration": 0.498793,
     "end_time": "2021-01-23T11:37:30.322343",
     "exception": false,
     "start_time": "2021-01-23T11:37:29.823550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_texts = [decoder_tokenizer.decode(i) for i in output_ids.detach().cpu().numpy()]\n",
    "predicted_texts = [decoder_tokenizer.decode(i) for i in logits.detach().cpu().numpy().argmax(axis=-1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T11:37:31.059781Z",
     "iopub.status.busy": "2021-01-23T11:37:31.058807Z",
     "iopub.status.idle": "2021-01-23T11:37:31.063818Z",
     "shell.execute_reply": "2021-01-23T11:37:31.064326Z"
    },
    "papermill": {
     "duration": 0.377529,
     "end_time": "2021-01-23T11:37:31.064488",
     "exception": false,
     "start_time": "2021-01-23T11:37:30.686959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text  [CLS] one of the other reviewers has mentioned that after watching just 1 oz episode you'll be hooked. they are right, as this is exactly what happened with me. the first thing that struck me about oz was its brutality and unflinching scenes of violence, which set in right from the word go. trust me, this is not a show for the faint hearted or timid. this show pulls no punches with regards to drugs, sex or violence. its is hardcore, in the classic use of the word. it is called oz as that is the nickname given to the oswald maximum security state penitentary. it focuses mainly on emerald city, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. em city is home to many.. aryans, muslims, gangstas, latinos, christians, italians, irish and more.... so scuffles, death stares, dodgy dealings and shady agreements are never far away. i would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. forget pretty pictures painted for mainstream audiences, forget charm, forget romance... oz doesn't mess around. the first episode i ever saw struck me as so nasty it was surreal, i couldn't say i was ready for it, but as i watched more, i developed a taste for oz, and got accustomed to the high levels of graphic violence. not just violence, but injustice ( crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience ) watching oz, you may become comfortable with what is uncomfortable viewing.... thats if you can get in touch with your darker side. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "predicted text  one of the other reviewers has mentioned that after watching just 1 oz episode you'll be hooked. they are right, as this is exactly what happened with me. the first thing that struck me about oz was its brutality and unflinching scenes of violence, which set in right from the word go. trust me, this is not a show for the faint hearted or timi. this show pulls no punches with regards to drugs, sex or violence. its is hardcore, in the classic use of the word. it is called oz as that is the nickname given to the oswald maximum security state penitentary. it focuses mainly on emerald city, an experimental section of the prisonson where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. em city is home city to many.. aryans, muslims, gangstas, latinos, italians, irish and irish and more.... so scufes, death stares, dodgy dealings and shady agreement are never far away away. i would would the the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. forget pretty pictures painted for mainstream audiences, forget charm, forget romance... oz doesn't mess aroundd. the first episode i ever saw struck me as so nasty it was surreal, i couldn't say i was ready for it, but as i watched more, i developed a taste for oz, and got accustomed to the high levels of graphic violence. not just just violence, but injustice ( crooked gards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience ) watching oz, you may become comfortable viewing what is uncomfortable viewing.... thats if you can get in touch with your darker side. [SEP] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print (\"original text \", original_texts[0])\n",
    "print (\"predicted text \", predicted_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T11:37:31.906032Z",
     "iopub.status.busy": "2021-01-23T11:37:31.869741Z",
     "iopub.status.idle": "2021-01-23T11:37:32.312687Z",
     "shell.execute_reply": "2021-01-23T11:37:32.312035Z"
    },
    "papermill": {
     "duration": 0.868127,
     "end_time": "2021-01-23T11:37:32.312808",
     "exception": false,
     "start_time": "2021-01-23T11:37:31.444681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_texts = []\n",
    "\n",
    "for i in best_val_logits:\n",
    "    for j in i:\n",
    "        text = decoder_tokenizer.decode(j).split()\n",
    "        text = [k for k in text if k not in ['[CLS]','[SEP]','[PAD]']]\n",
    "        predicted_texts.append(\" \".join(text).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T11:37:33.057943Z",
     "iopub.status.busy": "2021-01-23T11:37:33.057149Z",
     "iopub.status.idle": "2021-01-23T11:37:33.066436Z",
     "shell.execute_reply": "2021-01-23T11:37:33.066938Z"
    },
    "papermill": {
     "duration": 0.388317,
     "end_time": "2021-01-23T11:37:33.067083",
     "exception": false,
     "start_time": "2021-01-23T11:37:32.678766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val['predicted_text'] = predicted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T11:37:33.821708Z",
     "iopub.status.busy": "2021-01-23T11:37:33.820933Z",
     "iopub.status.idle": "2021-01-23T11:37:34.472404Z",
     "shell.execute_reply": "2021-01-23T11:37:34.471369Z"
    },
    "papermill": {
     "duration": 1.030526,
     "end_time": "2021-01-23T11:37:34.472581",
     "exception": false,
     "start_time": "2021-01-23T11:37:33.442055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import  nltk.translate.bleu_score as bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T11:37:35.211316Z",
     "iopub.status.busy": "2021-01-23T11:37:35.210382Z",
     "iopub.status.idle": "2021-01-23T11:37:35.213242Z",
     "shell.execute_reply": "2021-01-23T11:37:35.212730Z"
    },
    "papermill": {
     "duration": 0.383119,
     "end_time": "2021-01-23T11:37:35.213352",
     "exception": false,
     "start_time": "2021-01-23T11:37:34.830233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def WRR(text1,text2):\n",
    "    a = set(text1.lower().split())\n",
    "    b = set(text2.lower().split())\n",
    "    \n",
    "    if (len(a) == 0) and (len(b) == 0):\n",
    "        return .5\n",
    "    \n",
    "    c = a.intersection(b)\n",
    "    return float(len(c))/(len(a) + len(b) - len(c))\n",
    "\n",
    "def levenshtein(seq1, seq2):\n",
    "    seq1 = seq1.lower()\n",
    "    seq2 = seq2.lower()\n",
    "    \n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "    #print (matrix)\n",
    "    return (matrix[size_x - 1, size_y - 1])\n",
    "\n",
    "def CRR(text1, text2):\n",
    "    try:\n",
    "        return 1 - float(levenshtein(text1,text2))/max(len(text1),len(text2))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def bleu_score(text1,text2):\n",
    "    return bleu.sentence_bleu([text1.lower().split()],text2.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T11:37:35.942888Z",
     "iopub.status.busy": "2021-01-23T11:37:35.941894Z",
     "iopub.status.idle": "2021-01-23T11:37:38.096623Z",
     "shell.execute_reply": "2021-01-23T11:37:38.095281Z"
    },
    "papermill": {
     "duration": 2.523967,
     "end_time": "2021-01-23T11:37:38.096761",
     "exception": false,
     "start_time": "2021-01-23T11:37:35.572794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "val['text_len'] = val.original.apply(lambda x: len(encoder_tokenizer.encode(x, max_length=512, add_special_tokens=True)))\n",
    "val = val[val.text_len < MAX_LEN].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T11:37:38.848300Z",
     "iopub.status.busy": "2021-01-23T11:37:38.847237Z",
     "iopub.status.idle": "2021-01-23T11:37:43.993693Z",
     "shell.execute_reply": "2021-01-23T11:37:43.992272Z"
    },
    "papermill": {
     "duration": 5.546363,
     "end_time": "2021-01-23T11:37:43.993816",
     "exception": false,
     "start_time": "2021-01-23T11:37:38.447453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "val['WRR_1'] = val.apply(lambda x: WRR(x.original, x.solution), axis=1)\n",
    "val['WRR_2'] = val.apply(lambda x: WRR(x.predicted_text, x.solution), axis=1)\n",
    "\n",
    "#val['CRR_1'] = val.apply(lambda x: CRR(x.original, x.solution), axis=1)\n",
    "#val['CRR_2'] = val.apply(lambda x: CRR(x.predicted_text, x.solution), axis=1)\n",
    "\n",
    "val['BLEU_1'] = val.apply(lambda x: bleu_score(x.original, x.solution), axis=1)\n",
    "val['BLEU_2'] = val.apply(lambda x: bleu_score(x.predicted_text, x.solution), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T11:37:44.709889Z",
     "iopub.status.busy": "2021-01-23T11:37:44.709143Z",
     "iopub.status.idle": "2021-01-23T11:37:45.213158Z",
     "shell.execute_reply": "2021-01-23T11:37:45.211478Z"
    },
    "papermill": {
     "duration": 0.868607,
     "end_time": "2021-01-23T11:37:45.213331",
     "exception": false,
     "start_time": "2021-01-23T11:37:44.344724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val.to_csv('outputs_delete_augumented.csv',encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T11:37:45.921502Z",
     "iopub.status.busy": "2021-01-23T11:37:45.920605Z",
     "iopub.status.idle": "2021-01-23T11:37:45.950567Z",
     "shell.execute_reply": "2021-01-23T11:37:45.951102Z"
    },
    "papermill": {
     "duration": 0.388476,
     "end_time": "2021-01-23T11:37:45.951225",
     "exception": false,
     "start_time": "2021-01-23T11:37:45.562749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WRR_1</th>\n",
       "      <th>WRR_2</th>\n",
       "      <th>BLEU_1</th>\n",
       "      <th>BLEU_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1429.000000</td>\n",
       "      <td>1429.000000</td>\n",
       "      <td>1429.000000</td>\n",
       "      <td>1429.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.705396</td>\n",
       "      <td>0.820582</td>\n",
       "      <td>0.659489</td>\n",
       "      <td>0.807303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.085960</td>\n",
       "      <td>0.084699</td>\n",
       "      <td>0.115194</td>\n",
       "      <td>0.103078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.497487</td>\n",
       "      <td>0.144938</td>\n",
       "      <td>0.293268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.661654</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.597754</td>\n",
       "      <td>0.750581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.715328</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.672388</td>\n",
       "      <td>0.822889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.763736</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.741253</td>\n",
       "      <td>0.879113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.913514</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.913257</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             WRR_1        WRR_2       BLEU_1       BLEU_2\n",
       "count  1429.000000  1429.000000  1429.000000  1429.000000\n",
       "mean      0.705396     0.820582     0.659489     0.807303\n",
       "std       0.085960     0.084699     0.115194     0.103078\n",
       "min       0.268293     0.497487     0.144938     0.293268\n",
       "25%       0.661654     0.770492     0.597754     0.750581\n",
       "50%       0.715328     0.827586     0.672388     0.822889\n",
       "75%       0.763736     0.878788     0.741253     0.879113\n",
       "max       0.913514     1.000000     0.913257     1.000000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val[['WRR_1', 'WRR_2', 'BLEU_1', 'BLEU_2']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.35381,
     "end_time": "2021-01-23T11:37:46.662559",
     "exception": false,
     "start_time": "2021-01-23T11:37:46.308749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T11:37:47.387038Z",
     "iopub.status.busy": "2021-01-23T11:37:47.386139Z",
     "iopub.status.idle": "2021-01-23T11:37:47.392448Z",
     "shell.execute_reply": "2021-01-23T11:37:47.392984Z"
    },
    "papermill": {
     "duration": 0.361823,
     "end_time": "2021-01-23T11:37:47.393105",
     "exception": false,
     "start_time": "2021-01-23T11:37:47.031282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1372, 8) (1386, 8)\n"
     ]
    }
   ],
   "source": [
    "print (val[val.WRR_1 < val.WRR_2].shape, val[val.BLEU_1 < val.BLEU_2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.932247,
     "end_time": "2021-01-23T11:37:48.715039",
     "exception": false,
     "start_time": "2021-01-23T11:37:47.782792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T11:37:49.577716Z",
     "iopub.status.busy": "2021-01-23T11:37:49.576841Z",
     "iopub.status.idle": "2021-01-23T11:37:50.387254Z",
     "shell.execute_reply": "2021-01-23T11:37:50.386684Z"
    },
    "papermill": {
     "duration": 1.169378,
     "end_time": "2021-01-23T11:37:50.387371",
     "exception": false,
     "start_time": "2021-01-23T11:37:49.217993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: outputs.csv: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!cat outputs.csv | head -3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "papermill": {
   "duration": 25170.47452,
   "end_time": "2021-01-23T11:37:52.262152",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-23T04:38:21.787632",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01510d6d8e09434387f56e22f1c7c162": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0155f1bfdbc14d849dbcaa67bf4b911b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "044e894f16794a049ce1a3d03b9d8ef5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "061c2710bede4a5eaace696bfc0fc1ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e41290b248c2468f81e4d7afc74267e4",
       "max": 433,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f5bab91b453b495ca36ace0582c19add",
       "value": 433
      }
     },
     "10de7cd579064029a57d518e6e98666b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "222dfbe3cee64b32b0c318c5b304d349": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3e96e755885b477aaa1c69d4f947eda2",
       "placeholder": "​",
       "style": "IPY_MODEL_e53ff2307a0f45728094b1939d04e3ef",
       "value": " 232k/232k [00:00&lt;00:00, 575kB/s]"
      }
     },
     "227c3fda41d14589a92fc3f623be87e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "280600c1258b4b37a39f91594b4bc860": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_efbf561919d446fd87275171e02d19bc",
       "max": 466062,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cdefebaf3f5c48878426bd7e2877c490",
       "value": 466062
      }
     },
     "34bf0e7164234cf09f45180e70c2c7da": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3e328f06d7464078bf296121a2fa766b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_061c2710bede4a5eaace696bfc0fc1ff",
        "IPY_MODEL_562d3ccc55ab4115876ab2cb52cb77f3"
       ],
       "layout": "IPY_MODEL_b408b83ae1944b9fb47ddb5ffb49a40a"
      }
     },
     "3e96e755885b477aaa1c69d4f947eda2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4edea61c7ffc4288b85556837315512f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "506f31043dcc43eeb2639c5fba53f421": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4edea61c7ffc4288b85556837315512f",
       "placeholder": "​",
       "style": "IPY_MODEL_50ebba2fd2324034a721617bda20b3a2",
       "value": " 440M/440M [3:05:33&lt;00:00, 39.6kB/s]"
      }
     },
     "50ebba2fd2324034a721617bda20b3a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "562d3ccc55ab4115876ab2cb52cb77f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0155f1bfdbc14d849dbcaa67bf4b911b",
       "placeholder": "​",
       "style": "IPY_MODEL_8557739a4c9c4202834ced0d0e3bd5f7",
       "value": " 433/433 [00:08&lt;00:00, 51.8B/s]"
      }
     },
     "684835c7efe94fc9b265d35682f0a097": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_01510d6d8e09434387f56e22f1c7c162",
       "max": 440473133,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7ff2be5fb4f1443aabf633b9e4d5f4d4",
       "value": 440473133
      }
     },
     "76c2e3b7142c4b50887eb80a2e704531": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ea9f1472a7b9430b95defdfc437045ec",
       "max": 231508,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f92425a84d7d48dcb932575650f6503d",
       "value": 231508
      }
     },
     "79eddc1e815049bd8bb77183a14cfbb6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_684835c7efe94fc9b265d35682f0a097",
        "IPY_MODEL_506f31043dcc43eeb2639c5fba53f421"
       ],
       "layout": "IPY_MODEL_227c3fda41d14589a92fc3f623be87e0"
      }
     },
     "7ff2be5fb4f1443aabf633b9e4d5f4d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "8557739a4c9c4202834ced0d0e3bd5f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9f1e589d530d42e19750ce7d96c967b4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a19cac54101c4937a82cefc109357463": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_34bf0e7164234cf09f45180e70c2c7da",
       "placeholder": "​",
       "style": "IPY_MODEL_044e894f16794a049ce1a3d03b9d8ef5",
       "value": " 466k/466k [00:07&lt;00:00, 59.1kB/s]"
      }
     },
     "b408b83ae1944b9fb47ddb5ffb49a40a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cbc158a2e148456e9449f7ce64306a44": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_76c2e3b7142c4b50887eb80a2e704531",
        "IPY_MODEL_222dfbe3cee64b32b0c318c5b304d349"
       ],
       "layout": "IPY_MODEL_10de7cd579064029a57d518e6e98666b"
      }
     },
     "cdefebaf3f5c48878426bd7e2877c490": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "e41290b248c2468f81e4d7afc74267e4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e53ff2307a0f45728094b1939d04e3ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ea9f1472a7b9430b95defdfc437045ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "efbf561919d446fd87275171e02d19bc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f5bab91b453b495ca36ace0582c19add": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "f8226114751a466fa7af7f5d9d393c45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_280600c1258b4b37a39f91594b4bc860",
        "IPY_MODEL_a19cac54101c4937a82cefc109357463"
       ],
       "layout": "IPY_MODEL_9f1e589d530d42e19750ce7d96c967b4"
      }
     },
     "f92425a84d7d48dcb932575650f6503d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
